{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6dbfe76",
   "metadata": {},
   "source": [
    "# AI Database Performance Comparicon For AI Workloads: PostgreSQL/PgVector vs MongoDB Atlas Vector Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7920e1",
   "metadata": {},
   "source": [
    "```\n",
    "Note: This resource is intended to provide performance guidance for AI workloads using vector data within databases, this resoruce is not meant to be an official or comprehensive benchmark, but a guide to help you understand the performance characteristics of the databases within specific search patterns and workloads, enabling you to make an informed decision on which database to use for your AI workload.\n",
    "\n",
    "Because a database can has been traditionally used for a specific workload, doesn't mean that the database is the best fit for the workload.\n",
    "```\n",
    "\n",
    "This notebook doesn't provide:\n",
    "- A comprehensive benchmark fo all databases\n",
    "- A cost analysis for the databases and workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5a6a17",
   "metadata": {},
   "source": [
    "### Introduction:\n",
    "Welcome to this comprehensive notebook, where we provide performance insights for MongoDB and PostgreSQL - two of the most widely used databases in AI workloads.\n",
    "\n",
    "In this session we analyse the performance results of a variety of search mechanisms, including:\n",
    "- Vector Search\n",
    "- Hybrid Search\n",
    "\n",
    "**What You'll Learn:**\n",
    "- PostgreSQL with pgvector:\n",
    "  - How to set up a PostgreSQL database with the pgvector extension.\n",
    "  - How to run text, vector and hybrid searches on PostgreSQL.\n",
    "- MongoDB Atlas Vector Search:\n",
    "  - How to set up a MongoDB Atlas database with native Vector Search capabilities.\n",
    "    - How to execute text, vector and hybrid searches on MongoDB Atlas.\n",
    "- AI Workload Overview:\n",
    "  - This notebook showcases a standard AI workload involving vector embeddings and the retrieval of semantically simila rdocuments.\n",
    "  - The system leverages two different vector search solutions:\n",
    "    - PostgreSQL with pgvector: A powerful extension that integrates vector search capabilities directly into PostgreSQL.\n",
    "    - MongoDB Atlas Vector Search: A native vector search feature built into MongoDB, optimized for modern, document-based applications.\n",
    "- AI Workload Metrics:\n",
    "  - Latency: The tiem it takes to retrieve the top `n` results\n",
    "  - Throughput: The number of queriws processed per second\n",
    "  - P95 Latency: The 95th percentile latency of the queries\n",
    "\n",
    "**Database Platforms:**\n",
    "\n",
    "For this performance guidance, we utilize:\n",
    "- MongoDB Atlas: A fully managed, cloud-native database designed for modern applications.\n",
    "- Neon: A serverless, fully managed PostgreSQL database optimized for cloud deployments.\n",
    "\n",
    "Whether your focus is on MongoDB or PostgreSQl, this notebook is designed to help you understand their performance characteristics and guide you in achieving optimal performance for your AI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2594a069",
   "metadata": {},
   "source": [
    "## Key Information\n",
    "\n",
    "1. **System Configuration**\n",
    "\n",
    "### MongoDB Atlas (M30 → M40) vs. Neon (4 → 8 vCPUs) Comparison\n",
    "\n",
    "#### Important note on Resourse Allocation Disparities\n",
    "\n",
    "When interpreting the performance results in this notebook, it's essential to consider the significan tresource allocation differences between the tested systems:\n",
    "\n",
    "##### MongoDB Atlas (M30 → M40)\n",
    "- Minimum: 2 vCPUs, 8 GB RAM (M30)\n",
    "- Maximum: 4 vCPUs, 16 GB RAM (M40)\n",
    "\n",
    "##### Neon PosgreSQL\n",
    "- Minimum: 4 vCPUs, 16 GB RAM\n",
    "- Maximum: 8 vCPUs, 32 GB RAM\n",
    "\n",
    "This means Neon PosgreSQL has **twice the compute resources** at both minimum and maximum configurations compared to MongoDB Atlas. This resource disparity significantly impacts performance results interpretation in several ways:\n",
    "1. **Performance per Resource nit**: If MongoDB shows comparable or better performance despite having fewer resources, this suggests higher efficiency per compute unit.\n",
    "2. **Cost Considerations**: Higher resource allocation typically incures higher costs.\n",
    "3. **Scalling Behavior**: Both systems can scale, but across different resource ranges. Performance gains from scaling might manifest differently due to these distinct scaling ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6427294c",
   "metadata": {},
   "source": [
    "| Attribute                   | MongoDB Atlas (M30 → M40)                                                                 | Neon (Autoscaling: 4 → 8 vCPUs)                                                                 |\n",
    "|----------------------------|---------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|\n",
    "| **vCPUs**                  | - **Min**: M30 → 2 vCPUs (8 GB RAM)  <br> - **Max**: M40 → 4 vCPUs (16 GB RAM)              | - **Min**: 4 vCPUs (16 GB RAM) <br> - **Max**: 8 vCPUs (32 GB RAM)                              |\n",
    "| **Memory (RAM)**           | - **M30**: 8 GB <br> - **M40**: 16 GB                                                      | - **Min**: 16 GB <br> - **Max**: 32 GB                                                          |\n",
    "| **Storage**                | - **M30**: ~40 GB included <br> - **M40**: ~80 GB included <br> (Can scale further)         | - Remote \"pageserver\" stores primary data <br> - Local disk for temp files: 20 GB or 15 GB × 8 CUs (whichever is higher) |\n",
    "| **Autoscaling (Compute)**  | - **Cluster Tier Scaling**: can move between M30 and M40 <br> - **Storage Scaling**: automatically grows storage | - **Compute Autoscaling**: 4 to 8 vCPUs <br> - **Scale to Zero**: optional (after 5 min idle) |\n",
    "| **IOPS**                   | ~2000+ on M30, higher on M40                                                               | \"Burstable\" IOPS from cloud storage <br> Local File Cache for frequently accessed data         |\n",
    "| **Max Connections**        | - ~6000 (M30) <br> - ~12000 (M40)                                                          | - ~1802 (4 vCPUs) <br> - ~3604 (8 vCPUs)                                                       |\n",
    "| **Scale to Zero**          | Not supported                                                                              | Optional. If enabled, compute suspends when idle (adds startup latency)                        |\n",
    "| **Restart Behavior on Resizing** | - Moving from M30 to M40 triggers a brief re-provisioning <br> - Minimal downtime but connections can be interrupted | - Autoscaling within 4–8 vCPUs **does not** restart connections <br> - Editing min/max or toggling Scale to Zero triggers a restart |\n",
    "| **Local Disk for Temp Files** | Adequate for normal ops; M40 has more local disk                                        | At least 20 GB local disk, or 15 GB × 8 CUs = 120 GB if that's higher                          |\n",
    "| **Release Updates**        | - Minor updates auto-applied <br> - Major version upgrades can be scheduled               | - Weekly or scheduled updates <br> - Manual restart may be needed if Scale to Zero is disabled and you want the latest compute engine updates |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3990f70c",
   "metadata": {},
   "source": [
    "### Key Points\n",
    "- **Resource Range**\n",
    "  - MongoDB Atlas scales from 2 vCPUs/8 GB (M30) to 4 vCPUs/16 GB (M40)\n",
    "  - Neon ranges from 4 vCPUs/16 GB to 8 vCPUs/32 GB\n",
    "- **Closer Parity at M40**\n",
    "  - When Atlas scales to M40, it matches Neon's minimum (4 vCPUs/16 GB), allowing more direct performance comparisons\n",
    "  - Neon can still go beyond M40, up to 8 vCPUs/32 GB, if workload spikes exceed M40 capacity\n",
    "- **IOPS and Connections**\n",
    "  - Atlas M30→M40 has hogher IOPS and connection limits at each tier\n",
    "  - Neon's IOPS is cloud-based and \"burstable\", while connections scale with the CPU (CUs).\n",
    "\n",
    "In summary, MongoDB Atlas (M30 → M40) is closer to Neon (4 → 8 vCPUs) than previous tiers, especially at the high end (4 vCPUs/16 GB). However, Neon still offers more headroom if your workload demends exceed M40's capacity.\n",
    "\n",
    "2. **Data Processing**\n",
    "    - Uses Wikipedia dataset (100,000 entries) with embeddings (Precision: float32, Dimensions: 768) generated by Cohere\n",
    "    - JSON data is generated from the dataset and stored in the databases\n",
    "    - Stores data in both PostgreSQL and MongoDB\n",
    "3. **Performance Testing**\n",
    "    - Tests different sizes of concurrent queries (1-400 queries)\n",
    "    - Tests different insertion batch sizes and speed of insertion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f27443d",
   "metadata": {},
   "source": [
    "| Operation  | Metric        | Description                                                       |\n",
    "|------------|---------------|-------------------------------------------------------------------|\n",
    "| Insertion  | Latency       | Time taken to insert the data (average response time)            |\n",
    "|            | Throughput    | Number of queries processed per second                           |\n",
    "| Retrieval  | Latency       | Time taken to retrieve the top n results (average response time) |\n",
    "|            | Throughput    | Number of queries processed per second                           |\n",
    "|            | P95 Latency   | Time taken to retrieve the top n results for 95% of the queries  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4e6a69",
   "metadata": {},
   "source": [
    "4. **Results Visualisation**\n",
    "    - Interactive animations showing request-response cycles\n",
    "    - Comparative charts for latency and throughput\n",
    "    - Performance analysis across different batch sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9303ba6d",
   "metadata": {},
   "source": [
    "## Part 1: Data Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a85bd4e",
   "metadata": {},
   "source": [
    "Setting up the performance results dictionary `performance_guidance_results` and the batch sizes to test `CONCURRENT_QUERIES` and `TOTAL_QUERIES`\n",
    "- `performance_guidance_results` is a dictionary that will store the results of the tests\n",
    "- `CONCURRENT_QUERIES` is a list of the number of queries that are run concurrently\n",
    "- `TOTAL_QUERIES` is the total number of queries that are run\n",
    "\n",
    "Performance guidance Configuration Example: When testing with a concurrency level of 10:\n",
    "- We run 100 iterations\n",
    "- Each iteration runs 10concurrent queris\n",
    "- Total queries = 1,000 queries (TOTAL_ITERATIONS*CONCURRENT_QUERIES)\n",
    "\n",
    "NOTE: For each concurrency level in CONCURRENT_QUERIES:\n",
    "1. Run TOTAL_QUERIES iterations\n",
    "2. In each iteration, execute that many cuncurent queries\n",
    "3. Measure and collect latencies for all queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d76935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the performance_guidance_results results dictionary\n",
    "performance_guidance_results = {\"PostgreSQL\": {}, \"MongoDB\": {}}\n",
    "\n",
    "# The concurrency levels for performance guiddance testing\n",
    "# Each level represents the number of simulteneous queries to execute\n",
    "# This is important to note for AI workloads as it will affect the performance of the system as the number of requests increase\n",
    "CONCURRENT_QUERIES = [\n",
    "    1,\n",
    "    2,\n",
    "    4,\n",
    "    5,\n",
    "    8,\n",
    "] # 24, 32, 40, 48, 50, 56, 64, 72, 80, 88, 96, 100, 200, 400\n",
    "\n",
    "# The total number of iterations to run for each concurrency level\n",
    "# Thi sis important to note for AI workloads as it will affect the performance of the system as the number of queries per request increases\n",
    "TOTAL_QUERIES = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2444b9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# Function to securely get and set environment variables\n",
    "def set_env_securely(var_name, prompt):\n",
    "    value = getpass.getpass(prompt)\n",
    "    os.environ[var_name] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13f9b86",
   "metadata": {},
   "source": [
    "### Step 1: Install Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f08f981",
   "metadata": {},
   "source": [
    "All the libraries are installed using pip and facilitate the sourcing of data, embedding generation, and data visualization.\n",
    "- `datasets`: Hugging Face library for managing and preprocessing datasets across text, image and audio (https://huggingface.co/datasets)\n",
    "- `sentence_transformers`: For creating sentence embeddings for tasks like semantic search and clustering (https://www.sbert.net/)\n",
    "- `pandas`: A library for data manipulation and analysis with DataFrames and Series (https://pandas.pydata.org/)\n",
    "- `matplotlib`: A library for creating static, interactive and animated data visualizations (https://matplotlib.org/)\n",
    "- `seaborn`: A library for creating statistical data visualizations (https://seaborn.pydata.org/)\n",
    "- `cohere`: A library for generating embeddings and accessing the Cohere API or models (https://cohere.ai/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4c6a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade datasets sentence_transformers pandas matplotlib seaborn cohere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5d9760",
   "metadata": {},
   "source": [
    "### Step 2: Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a6c52a",
   "metadata": {},
   "source": [
    "The dataset for the notebook is sourced from the Hugging Face Cohere Wikipedia dataset.\n",
    "\n",
    "The [Cohere/wikipedia-22-12-en-embeddings](https://huggingface.co/datasets/Cohere/wikipedia-22-12-en-embeddings) dataset on Hugging Face comprises English Wikipedia articles embedded using Cohere's multilingual-22-12 model. Each entry includes the article's title, text, URL, Wikipedia ID, view count, paragraph ID, language codes, and a 768-dimensional embedding vector. This dataset is valuable for tasks like semantic search, information retrieval and NLP model training.\n",
    "\n",
    "For this notebook we are using 100,000 rows of the dataset and have removed the id, wiki_id, paragraph_id, langs and view columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bed2516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Using 100,000 rows for testing feel free to change this to any number of rows you want to test\n",
    "# The wikipedia-22-12-en-embeddings dataset has approximately 35,000,000 rows and requires 120GB of memory to load\n",
    "MAX_ROWS = 100000\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"Cohere/wikipedia-22-12-en-embeddings\", split=\"train\", streaming=True\n",
    ")\n",
    "dataset_segment = dataset.take(MAX_ROWS)\n",
    "\n",
    "# Convert the dataset to a pandas dataframe\n",
    "dataset_df = pd.DataFrame(dataset_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cac2bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a JSON attribute to the dataset consisting of the title, text and url\n",
    "dataset_df[\"json_data\"] = dataset_df.apply(\n",
    "    lambda row: {\"title\": row[\"title\"], \"text\": row[\"text\"], \"url\": row[\"url\"]}, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f345bd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the id field, wiki_id, paragraph_id, langs and views from the dataset\n",
    "# This is to replicate the structure of dataset usually encountered in AI workloads, particularly particularly in RAG systems where metadata is extracted from documents and stored.\n",
    "dataset_df = dataset_df.drop(\n",
    "    columns=[\"id\", \"wiki_id\", \"paragraph_id\", \"langs\", \"views\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398292ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the emb column name to embedding\n",
    "dataset_df = dataset_df.rename(columns={\"emb\": \"embedding\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6298b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee5ac63",
   "metadata": {},
   "source": [
    "### Step 3: Embedding Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333eb9c6",
   "metadata": {},
   "source": [
    "We use the Cohere API to generate embeddings for the test queries.\n",
    "\n",
    "To get the Cohere API key, you can sign up for a free account on the Cohere website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dd760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Cohere API key\n",
    "set_env_securely(\"COHERE_API_KEY\", \"Enter your Cohere API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07c8f9c",
   "metadata": {},
   "source": [
    "Using the Cohere API to generate embeddings for the test queries.\n",
    "\n",
    "Using the `embed_multilingual-v2.0` model. This is the same model used in the Cohere Wikipedia dataset.\n",
    "\n",
    "Embedding size is 768 dimensions and the precision is float32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9f7ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import cohere\n",
    "\n",
    "# Initialize Cohere Client\n",
    "co = cohere.Client()\n",
    "\n",
    "\n",
    "def get_cohere_embeddings(\n",
    "        sentences: List[str],\n",
    "        model: str = \"embed-multilingual-v2.0\",\n",
    "        input_type: str = \"search_document\",\n",
    ")-> Tuple[List[float], List[int]]:\n",
    "    \"\"\"\n",
    "    Generate embeddings for the provided sentences using Cohere's embedding model.\n",
    "\n",
    "    Args:\n",
    "    sentences (list of str): List of sentences to generate embeddings for.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[List[float], List[int]]: A tuple containing two lists of embeddings (float and int8).\n",
    "    \"\"\"\n",
    "    generated_embedding = co.embed(\n",
    "        texts=sentences,\n",
    "        model=\"embed-multilingual-v2.0\",\n",
    "        input_type=\"search_document\",\n",
    "        embedding_types=[\"float\"],\n",
    "    ).embeddings\n",
    "\n",
    "    return generated_embedding.float[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cac91ca",
   "metadata": {},
   "source": [
    "Generate embeddings for the query templates\n",
    "\n",
    "Store the embeddings in a dictionary for easy access\n",
    "\n",
    "Note: Doing this to avoid tje overhead of generating embeddings for each query in the dataset during the performance analysis process, as this is a time consuming process and expensive in terms of API usage.\n",
    "\n",
    "Note: Feel free to add more queries to the query_templates list to test the performance of the vector database with a larger number of queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a753a18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_templates = [\n",
    "    \"When was YouTube officially launched, and by whom?\",\n",
    "    \"What is YouTube's slogan introduced after Google's acquisition?\",\n",
    "    \"How many hours of videos are collectively watched on YouTube daily?\",\n",
    "    \"Which was the first video uploaded to YouTube, and when was it uploaded?\",\n",
    "    \"What was the acquisition cost of YouTube by Google, and when was the deal finalized?\",\n",
    "    \"What was the first YouTube video to reach one million views, and when did it happen?\",\n",
    "    \"What are the three separate branches of the United States government?\",\n",
    "    \"Which counrty has the highest documented incarceration rate and prison population?\",\n",
    "    \"How many executions have occured in the United States since 1977, and which countries have more?\",\n",
    "    \"What percentage of the global military spending did the United States account for in 2019?\",\n",
    "    \"How is the U.S. president elected?\",\n",
    "    \"What cooling system innovation was included in the proposed venues for the World Cup in Qatar?\",\n",
    "    \"What lawsuit was giled against Google in June 2022, and what was it about?\",\n",
    "    \"How much was Google fined by CNIL in January 2022, and for what reason?\",\n",
    "    \"When did YouTube join the NSA's PRISM program, according to reports?\",    \n",
    "]\n",
    "\n",
    "# For each query template question generate the embedding\n",
    "# NOTE: Doing this to avoid the overhead of generating embeddings for each query during the performance comparison\n",
    "query_embeddings = [\n",
    "    get_cohere_embeddings(sentences=[query], input_type=\"search_query\")\n",
    "    for query in query_templates\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03687dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with the query templates and their corresponding embeddings\n",
    "query_embeddings_dict = {\n",
    "    query: embedding for query, embedding in zip(query_templates, query_embeddings)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd99d649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first 5 query embeddings as a dataframe\n",
    "pd.DataFrame(query_embeddings_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b70c2a5",
   "metadata": {},
   "source": [
    "## Part 2: Retrieval Mechanisms with PostgreSQL and PgVector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b873ffe2",
   "metadata": {},
   "source": [
    "In this section we create a PostgreSQL database with the PgVector extention and insert the dataset into the database.\n",
    "\n",
    "We are also going to implement various search mechanisms on the databases to test the performance of the database under certain search patterns and workloads. Specifically we are going to implement a semantic search mechanism on the database via vector sesarch and hybrid search mechanism on the database via vector search and text search.\n",
    "\n",
    "The table `wikipedia_data` is created with the following columns:\n",
    "- `id`: The unique identifier for each row\n",
    "- `title`: The title of the Wikipedia article \n",
    "- `text`: The text of the Wikipedia article\n",
    "- `url`: The URL of the Wikipedia article\n",
    "- `json_data`: The JSON data of the Wikipedia article\n",
    "- `embedding`: The embedding vector for the Wikipedia article\n",
    "\n",
    "The table is created with HNSW index with m=16, ef_construction=64 and cosine similarity (these are the default parameters for the HNSW index in pgvector).\n",
    "- `HNSW`: Hierarchical Navigable Small World graphs are a type of graph-based index that are used for efficient similarity search.\n",
    "- `m=16`: The number of edges per node in the graph\n",
    "- `ef_construction=64`: Short for exploration factor construction, is the number of edges to build during the index construction phase\n",
    "- `ef_search=100`: Short for exploration factor search, is the number of edges to search during the index search phase\n",
    "- `cosine similarity`: The similarity metric used for the index (formula: dot product(A,B)/(|A||B|))\n",
    "- `cosine distance`: The distance metric calculated using cosine similarity (1 - cosine similarity)\n",
    "\n",
    "We perform a semantic search on the database using a single data point of the query templates and their corresponding embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fc18e1",
   "metadata": {},
   "source": [
    "### Step 1: Install Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f682c19a",
   "metadata": {},
   "source": [
    "- `pgvector` (0.3.6): A PostgreSQL extension for vector similarity search (https://github.com/pgvector/pgvector)\n",
    "- `psycopg` (3.2.3): A PostgreSQL database adapter for Python (https://www.psycopg.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ca5331",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pgvector \"psycopg[binary]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4368b8",
   "metadata": {},
   "source": [
    "### Step 2: Create Posgres Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7126e1ab",
   "metadata": {},
   "source": [
    "- `id`: The unique identifier for each row\n",
    "- `title`: The title of the Wikipedia article \n",
    "- `text`: The text of the Wikipedia article\n",
    "- `url`: The URL of the Wikipedia article\n",
    "- `json_data`: The JSON data of the Wikipedia article\n",
    "- `embedding`: The embedding vector for the Wikipedia article\n",
    "\n",
    "**Key aspect of PostgreSQL table creation:**\n",
    "- `id`: The unique identifier for each row stored with the data type `bigserial` which is a 64-bit integer and auto-incremented.\n",
    "- `title`: The title of the Wikipedia article stored with the data type `text` which is a variable character string.\n",
    "- `text`: The text of the Wikipedia article stored with the data type `text` which is a variable character string.\n",
    "- `url`: The URL of the Wikipedia article stored with the data type `text` which is a variable character string.\n",
    "- `json_data`: The JSON data of the Wikipedia article stored with the data type `jsonb` which is a binary formatted JSON data type.\n",
    "- `embedding`: The embedding vector for the Wikipedia article stored with the data type `vector(768)` which is a provided by pgvector and is of 768 dimentions.\n",
    "\n",
    "**Optimizing the table for search:**\n",
    "- `search vector`: The search vector for the Wikipedia article stored with the data type `tsvector` which is a text search data type in PostgreSQL.\n",
    "- The expression inside the `GENERATED ALWAYS AS` clause is the text (title and text) to be tokenized and indexed for full-text search.\n",
    "- Using `coalesce` to handle any null values in the title or text columns.\n",
    "- `STORED`: The keyword indicates that the `search_vector` column is stored in the table, this avoids the overhead of recalculating the `search_vector` column during queries, and improves performance.\n",
    "\n",
    "**Extra**\n",
    "- The `serach_vector` column is computed automatically using the text in the `title` and `text` fields, making full-text search more efficient by avoiding on-the-fly computation.\n",
    "- The `HNSW` index on the `embedding` column is optimized for ANN queries using cosine similarity, which is crucial for semantic search.\n",
    "- The `GIN` indexes on both the `json_data` and `search_vector` columns ensure fast query performance on JSONB queries and full-text search, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cf69f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(connection):\n",
    "    with connection.cursor() as cur:\n",
    "        # Drop table if it already exists\n",
    "        cur.execute(\"DROP TABLE IF EXISTS wikipedia_data\")\n",
    "\n",
    "        # Create the table with the appropriate structure\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE TABLE wikipedia_data (\n",
    "                id bigserial PRIMARY kEY,\n",
    "                title text,\n",
    "                text text,\n",
    "                url text,\n",
    "                json_data jsonb,\n",
    "                embedding vector(768),\n",
    "                search_vector tsvector GENERATED ALWAYS AS (\n",
    "                    to_tsvector('english', coalesce(title, '') || ' ' || coalesce(text, ''))\n",
    "                ) STORED\n",
    "            )\n",
    "        \"\"\")\n",
    "        # Create HNSW index for vectorsimilarity search with cosine similarity\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE INDEX wikipedia_data_embedding_hnsw_idx\n",
    "            ON wikipedia_data\n",
    "            USING hnsw (embedding_vector_cosine_ops)\n",
    "            WITH (m = 16, ef_construction = 64);\n",
    "        \"\"\")\n",
    "\n",
    "        # Create GIN index on the json_data column for efficient JSONB queruing\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE INDEX wikipedia_data_embedding_gin_idx\n",
    "            ON wikipedia_data\n",
    "            USING GIN (json_data);\n",
    "        \"\"\")\n",
    "\n",
    "        # Create GIN index on the search_vector column for efficient full-text search\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE INDEX wikipedia_data_search_vector_idx\n",
    "            ON wikipedia_data\n",
    "            USING GIN (search_vector);\n",
    "        \"\"\")\n",
    "\n",
    "        print(\"Table and indexes created successfully\")\n",
    "        connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148600d0",
   "metadata": {},
   "source": [
    "### Step 4: Define insert function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd503c3a",
   "metadata": {},
   "source": [
    "For inserting JSON data, we convert the Python Dictionary in the `json_data` attribute to a JSON string using the `json.dumps()` function. \n",
    "\n",
    "This is a serialization process that converts the Python Dictionary in the `json_data` attribute to a JSON string that is stored as binary data in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fcf18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def insert_data_to_postgres(dataframe, connection, database_type=\"PostgreSQL\"):\n",
    "    \"\"\"\n",
    "    Insert data into the PostgreSQL database.\n",
    "\n",
    "    Args:\n",
    "    dataframe (pandas.DataFrame): The dataframe containing the data to insert.\n",
    "    connection (psycopg.extensions.connection): The connection to the PostgreSQL database.\n",
    "    database_type (str): The type of database (default: \"PostgreSQL\"). \n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    total_rows = len(dataframe)\n",
    "\n",
    "    try:\n",
    "        with connection.cursor() as cur:\n",
    "            # Create a list of tuples for insertion, filtering out rows with invalid embeddings\n",
    "            data_tuples = []\n",
    "            for _, row in dataframe.iterrows():\n",
    "                data_tuple = (\n",
    "                    row[\"title\"],\n",
    "                    row[\"text\"],\n",
    "                    row[\"url\"],\n",
    "                    json.dumps(row[\"json_data\"]), # convert dict to JSON string\n",
    "                    row[\"embedding\"],\n",
    "                )\n",
    "                data_tuples.append(data_tuple)\n",
    "\n",
    "            if not data_tuples:\n",
    "                raise ValueError(\"No valid data tuples to insert\")\n",
    "            \n",
    "            cur.executemary(\n",
    "                \"\"\"\n",
    "                INSERT INTO wikipedia_data\n",
    "                (title, text, url, json_data, embedding)\n",
    "                VALUES (%s, %s, %s, %s, %s)\n",
    "                \"\"\",\n",
    "                data_tuples,\n",
    "            )\n",
    "\n",
    "            connection.commit()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erroe during bulk insert: {e}\")\n",
    "        connection.rollback()\n",
    "        raise e\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    rows_per_second = len(data_tuples) / total_time\n",
    "\n",
    "    # print(f\"\\nInsertion Statistics:\")\n",
    "    # print(f\"Total time: {total_time:.2f} seconds\")\n",
    "    # print(f\"Average insertion rate: {rows_per_second:.2f} rows/second\")\n",
    "    # print(f\"Total rows inserted: {len(data_tuples)}\")\n",
    "    # print(f\"Rows skipped: {total_rows - len(data_tuples)}\")\n",
    "\n",
    "    # Store results in performance guidance dictionary\n",
    "    if database_type not in performance_guidance_results:\n",
    "        performance_guidance_results[database_type] = {}\n",
    "\n",
    "    performance_guidance_results[database_type][\"insert_time\"] = {\n",
    "        \"total_time\": total_time,\n",
    "        \"rows_per_second\": rows_per_second,\n",
    "        \"total_rows\": total_rows,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2e334e",
   "metadata": {},
   "outputs": [],
   "source": [
    "neon_db_host = os.environ[\"PGHOST\"]\n",
    "neon_db_database = os.environ[\"PGDATABASE\"]\n",
    "neon_db_user = os.environ[\"PGUSER\"]\n",
    "neon_db_password = os.environ[\"PGPASSWORD\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be4492d",
   "metadata": {},
   "source": [
    "### Step 5: Insert data into Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4798f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg\n",
    "from pgvector.psycopg import register_vector\n",
    "\n",
    "try:\n",
    "    # Connect to PostgreSQL\n",
    "    conn = psycopg.connect(\n",
    "        f\"dbname={neon_db_database} user={neon_db_user} password={neon_db_password} host={neon_db_host}\"\n",
    "    )\n",
    "\n",
    "    # Enable the pgvector extension\n",
    "    conn.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n",
    "\n",
    "    # Register vector type to handle embedding data as vector data types\n",
    "    register_vector(conn)\n",
    "\n",
    "    # Step 1: Create the table\n",
    "    create_table(conn)\n",
    "\n",
    "    # Step 2: Insert the expended dataset into the table\n",
    "    insert_data_to_postgres(dataset_df, conn)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Failed to execute: \", e)\n",
    "finally:\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "    print(\"Connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30829672",
   "metadata": {},
   "source": [
    "### Step 6: Define Text search function with Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5076f104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_search_with_postgres(query, connection, top_n=5):\n",
    "    \"\"\"\n",
    "    Perform a full-text search on the precomputed 'search_vector' column of the 'wikipedia_data' table. \n",
    "    \"\"\"\n",
    "    with connection.cursor() as cur:\n",
    "        # Convert the search query into a tsquery\n",
    "        cur.execute(\"SELECT plainto_tsquery('english', %s)\", (query,))\n",
    "        ts_query = cur.fetchone()[0]\n",
    "\n",
    "        # Execute the full-text search query using the precomputed search_vector column\n",
    "        cur.execute(\n",
    "            \"\"\" \n",
    "            SELECT title, text, url, json_data,\n",
    "                ts_rank_cd(search_vector, %s) AS rank\n",
    "            FROM wikipedia_data\n",
    "            WHERE search_vector @@ %s\n",
    "            ORDER BY rank DESC\n",
    "            LIMIT %s\n",
    "            \"\"\",\n",
    "            (ts_query, ts_query, top_n),\n",
    "        )\n",
    "\n",
    "        results = cur.fetchall()\n",
    "\n",
    "        formatted_results = [\n",
    "            {\n",
    "                \"title\": r[0],\n",
    "                \"text\": r[1],\n",
    "                \"url\": r[2],\n",
    "                \"json_data\": r[3],\n",
    "                \"rank\": r[4],\n",
    "            }\n",
    "            for r in results\n",
    "        ]\n",
    "\n",
    "        return formatted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07ac48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Connect to PostgreSQL\n",
    "    conn = psycopg.connect(\n",
    "        f\"dbname={neon_db_database} user={neon_db_user} password={neon_db_password} host={neon_db_host}\"\n",
    "    )\n",
    "\n",
    "    text_search_with_postgres_results = text_search_with_postgres(\n",
    "        \"When was Youtube officially launched, and by whom?\", conn\n",
    "    )\n",
    "    # Print results in a formatted way\n",
    "    for result in text_search_with_postgres_results:\n",
    "        print(f\"\\nTitle: {result}\")\n",
    "        print(f\"Text: {result['text']}\")\n",
    "        print(f\"URL: {result['url']}\")\n",
    "        print(f\"JSON Data: {result['json_data']}\")\n",
    "        print(f\"Rank: {result['rank']:.4f}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Failed to connect or execute query:\", e)\n",
    "finally:\n",
    "    conn.close()\n",
    "    print(\"Connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1626e3",
   "metadata": {},
   "source": [
    "### Step 7: Define vector search function with Postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262dd42d",
   "metadata": {},
   "source": [
    "To avoid exhasuting API key usage, we will fetch the query embedding from the `query_embeddings_dict` dictionary.\n",
    "\n",
    "In the `vector_search_with_postgres` function, we set the HNSW ef parameter to 100 using the `execute_command` function.\n",
    "\n",
    "This is to set the exploration factor for the HNSW index to 100. And corresponds to the number of nodes/candidates to search during the index search phase. A node corresponds to a vector in the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797f1190",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99f69306",
   "metadata": {},
   "source": [
    "### Step 8: Define hybrid search function with Postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5d6005",
   "metadata": {},
   "source": [
    "## Part 3: Retrieval Mechanisms with MongoDB Atlas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37870fea",
   "metadata": {},
   "source": [
    "### Step 1: Install Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630d4fb1",
   "metadata": {},
   "source": [
    "### Step 2: Create MongoDB Atlas Account"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655af09a",
   "metadata": {},
   "source": [
    "### Step 3: Connect to MongoDB and Create Database and Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eef367",
   "metadata": {},
   "source": [
    "### Step 4: Vector Index Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea428aa",
   "metadata": {},
   "source": [
    "### Step 5: Define Insert Data Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d6bdca",
   "metadata": {},
   "source": [
    "### Step 6: Insert Data into MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83fef7e",
   "metadata": {},
   "source": [
    "### Step 7: Define Text Search Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91199186",
   "metadata": {},
   "source": [
    "### Step 8: Define Vector Search Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bab7baf",
   "metadata": {},
   "source": [
    "### Step 9: Define Hybrid Search Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f1da8e",
   "metadata": {},
   "source": [
    "#### Note: \n",
    "In the MongoDB hybrid search function, two weights:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5be5de",
   "metadata": {},
   "source": [
    "## Part 4: Vector Database Performance Analysis Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b35f2b6",
   "metadata": {},
   "source": [
    "#### Insertion Performance Analysis Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b8ea2f",
   "metadata": {},
   "source": [
    "1.1 PosgreSQL Insertion Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecfb761",
   "metadata": {},
   "source": [
    "1.2 MongoDB Insertion Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d16b9d",
   "metadata": {},
   "source": [
    "1.3 Visualize Insertion Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0eabe0",
   "metadata": {},
   "source": [
    "#### Semantic Search with PostgreSQL and PgVector Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad99aba",
   "metadata": {},
   "source": [
    "2.2. MongoDB Semantic Search Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4e7560",
   "metadata": {},
   "source": [
    "2.3 Visualize Vector Search Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9298e80f",
   "metadata": {},
   "source": [
    "## Part 5: Extra Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cef1fce",
   "metadata": {},
   "source": [
    "5.1 PostgreSQL JSONB vs. MongoDB BSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe4a880",
   "metadata": {},
   "source": [
    "5.2 Limitations of pgvector for Handling Large-Dimensional Embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
