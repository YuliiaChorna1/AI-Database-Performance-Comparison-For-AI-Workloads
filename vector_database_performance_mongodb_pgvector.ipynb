{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6dbfe76",
   "metadata": {},
   "source": [
    "# AI Database Performance Comparicon For AI Workloads: PostgreSQL/PgVector vs MongoDB Atlas Vector Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7920e1",
   "metadata": {},
   "source": [
    "```\n",
    "Note: This resource is intended to provide performance guidance for AI workloads using vector data within databases, this resoruce is not meant to be an official or comprehensive benchmark, but a guide to help you understand the performance characteristics of the databases within specific search patterns and workloads, enabling you to make an informed decision on which database to use for your AI workload.\n",
    "\n",
    "Because a database can has been traditionally used for a specific workload, doesn't mean that the database is the best fit for the workload.\n",
    "```\n",
    "\n",
    "This notebook doesn't provide:\n",
    "- A comprehensive benchmark fo all databases\n",
    "- A cost analysis for the databases and workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5a6a17",
   "metadata": {},
   "source": [
    "### Introduction:\n",
    "Welcome to this comprehensive notebook, where we provide performance insights for MongoDB and PostgreSQL - two of the most widely used databases in AI workloads.\n",
    "\n",
    "In this session we analyse the performance results of a variety of search mechanisms, including:\n",
    "- Vector Search\n",
    "- Hybrid Search\n",
    "\n",
    "**What You'll Learn:**\n",
    "- PostgreSQL with pgvector:\n",
    "  - How to set up a PostgreSQL database with the pgvector extension.\n",
    "  - How to run text, vector and hybrid searches on PostgreSQL.\n",
    "- MongoDB Atlas Vector Search:\n",
    "  - How to set up a MongoDB Atlas database with native Vector Search capabilities.\n",
    "    - How to execute text, vector and hybrid searches on MongoDB Atlas.\n",
    "- AI Workload Overview:\n",
    "  - This notebook showcases a standard AI workload involving vector embeddings and the retrieval of semantically simila rdocuments.\n",
    "  - The system leverages two different vector search solutions:\n",
    "    - PostgreSQL with pgvector: A powerful extension that integrates vector search capabilities directly into PostgreSQL.\n",
    "    - MongoDB Atlas Vector Search: A native vector search feature built into MongoDB, optimized for modern, document-based applications.\n",
    "- AI Workload Metrics:\n",
    "  - Latency: The tiem it takes to retrieve the top `n` results\n",
    "  - Throughput: The number of queriws processed per second\n",
    "  - P95 Latency: The 95th percentile latency of the queries\n",
    "\n",
    "**Database Platforms:**\n",
    "\n",
    "For this performance guidance, we utilize:\n",
    "- MongoDB Atlas: A fully managed, cloud-native database designed for modern applications.\n",
    "- Neon: A serverless, fully managed PostgreSQL database optimized for cloud deployments.\n",
    "\n",
    "Whether your focus is on MongoDB or PostgreSQl, this notebook is designed to help you understand their performance characteristics and guide you in achieving optimal performance for your AI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2594a069",
   "metadata": {},
   "source": [
    "## Key Information\n",
    "\n",
    "1. **System Configuration**\n",
    "\n",
    "### MongoDB Atlas (M30 → M40) vs. Neon (4 → 8 vCPUs) Comparison\n",
    "\n",
    "#### Important note on Resourse Allocation Disparities\n",
    "\n",
    "When interpreting the performance results in this notebook, it's essential to consider the significan tresource allocation differences between the tested systems:\n",
    "\n",
    "##### MongoDB Atlas (M30 → M40)\n",
    "- Minimum: 2 vCPUs, 8 GB RAM (M30)\n",
    "- Maximum: 4 vCPUs, 16 GB RAM (M40)\n",
    "\n",
    "##### Neon PosgreSQL\n",
    "- Minimum: 4 vCPUs, 16 GB RAM\n",
    "- Maximum: 8 vCPUs, 32 GB RAM\n",
    "\n",
    "This means Neon PosgreSQL has **twice the compute resources** at both minimum and maximum configurations compared to MongoDB Atlas. This resource disparity significantly impacts performance results interpretation in several ways:\n",
    "1. **Performance per Resource nit**: If MongoDB shows comparable or better performance despite having fewer resources, this suggests higher efficiency per compute unit.\n",
    "2. **Cost Considerations**: Higher resource allocation typically incures higher costs.\n",
    "3. **Scalling Behavior**: Both systems can scale, but across different resource ranges. Performance gains from scaling might manifest differently due to these distinct scaling ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6427294c",
   "metadata": {},
   "source": [
    "| Attribute                   | MongoDB Atlas (M30 → M40)                                                                 | Neon (Autoscaling: 4 → 8 vCPUs)                                                                 |\n",
    "|----------------------------|---------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|\n",
    "| **vCPUs**                  | - **Min**: M30 → 2 vCPUs (8 GB RAM)  <br> - **Max**: M40 → 4 vCPUs (16 GB RAM)              | - **Min**: 4 vCPUs (16 GB RAM) <br> - **Max**: 8 vCPUs (32 GB RAM)                              |\n",
    "| **Memory (RAM)**           | - **M30**: 8 GB <br> - **M40**: 16 GB                                                      | - **Min**: 16 GB <br> - **Max**: 32 GB                                                          |\n",
    "| **Storage**                | - **M30**: ~40 GB included <br> - **M40**: ~80 GB included <br> (Can scale further)         | - Remote \"pageserver\" stores primary data <br> - Local disk for temp files: 20 GB or 15 GB × 8 CUs (whichever is higher) |\n",
    "| **Autoscaling (Compute)**  | - **Cluster Tier Scaling**: can move between M30 and M40 <br> - **Storage Scaling**: automatically grows storage | - **Compute Autoscaling**: 4 to 8 vCPUs <br> - **Scale to Zero**: optional (after 5 min idle) |\n",
    "| **IOPS**                   | ~2000+ on M30, higher on M40                                                               | \"Burstable\" IOPS from cloud storage <br> Local File Cache for frequently accessed data         |\n",
    "| **Max Connections**        | - ~6000 (M30) <br> - ~12000 (M40)                                                          | - ~1802 (4 vCPUs) <br> - ~3604 (8 vCPUs)                                                       |\n",
    "| **Scale to Zero**          | Not supported                                                                              | Optional. If enabled, compute suspends when idle (adds startup latency)                        |\n",
    "| **Restart Behavior on Resizing** | - Moving from M30 to M40 triggers a brief re-provisioning <br> - Minimal downtime but connections can be interrupted | - Autoscaling within 4–8 vCPUs **does not** restart connections <br> - Editing min/max or toggling Scale to Zero triggers a restart |\n",
    "| **Local Disk for Temp Files** | Adequate for normal ops; M40 has more local disk                                        | At least 20 GB local disk, or 15 GB × 8 CUs = 120 GB if that's higher                          |\n",
    "| **Release Updates**        | - Minor updates auto-applied <br> - Major version upgrades can be scheduled               | - Weekly or scheduled updates <br> - Manual restart may be needed if Scale to Zero is disabled and you want the latest compute engine updates |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3990f70c",
   "metadata": {},
   "source": [
    "### Key Points\n",
    "- **Resource Range**\n",
    "  - MongoDB Atlas scales from 2 vCPUs/8 GB (M30) to 4 vCPUs/16 GB (M40)\n",
    "  - Neon ranges from 4 vCPUs/16 GB to 8 vCPUs/32 GB\n",
    "- **Closer Parity at M40**\n",
    "  - When Atlas scales to M40, it matches Neon's minimum (4 vCPUs/16 GB), allowing more direct performance comparisons\n",
    "  - Neon can still go beyond M40, up to 8 vCPUs/32 GB, if workload spikes exceed M40 capacity\n",
    "- **IOPS and Connections**\n",
    "  - Atlas M30→M40 has hogher IOPS and connection limits at each tier\n",
    "  - Neon's IOPS is cloud-based and \"burstable\", while connections scale with the CPU (CUs).\n",
    "\n",
    "In summary, MongoDB Atlas (M30 → M40) is closer to Neon (4 → 8 vCPUs) than previous tiers, especially at the high end (4 vCPUs/16 GB). However, Neon still offers more headroom if your workload demends exceed M40's capacity.\n",
    "\n",
    "2. **Data Processing**\n",
    "    - Uses Wikipedia dataset (100,000 entries) with embeddings (Precision: float32, Dimensions: 768) generated by Cohere\n",
    "    - JSON data is generated from the dataset and stored in the databases\n",
    "    - Stores data in both PostgreSQL and MongoDB\n",
    "3. **Performance Testing**\n",
    "    - Tests different sizes of concurrent queries (1-400 queries)\n",
    "    - Tests different insertion batch sizes and speed of insertion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f27443d",
   "metadata": {},
   "source": [
    "| Operation  | Metric        | Description                                                       |\n",
    "|------------|---------------|-------------------------------------------------------------------|\n",
    "| Insertion  | Latency       | Time taken to insert the data (average response time)            |\n",
    "|            | Throughput    | Number of queries processed per second                           |\n",
    "| Retrieval  | Latency       | Time taken to retrieve the top n results (average response time) |\n",
    "|            | Throughput    | Number of queries processed per second                           |\n",
    "|            | P95 Latency   | Time taken to retrieve the top n results for 95% of the queries  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4e6a69",
   "metadata": {},
   "source": [
    "4. **Results Visualisation**\n",
    "    - Interactive animations showing request-response cycles\n",
    "    - Comparative charts for latency and throughput\n",
    "    - Performance analysis across different batch sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9303ba6d",
   "metadata": {},
   "source": [
    "## Part 1: Data Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a85bd4e",
   "metadata": {},
   "source": [
    "Setting up the performance results dictionary `performance_guidance_results` and the batch sizes to test `CONCURRENT_QUERIES` and `TOTAL_QUERIES`\n",
    "- `performance_guidance_results` is a dictionary that will store the results of the tests\n",
    "- `CONCURRENT_QUERIES` is a list of the number of queries that are run concurrently\n",
    "- `TOTAL_QUERIES` is the total number of queries that are run\n",
    "\n",
    "Performance guidance Configuration Example: When testing with a concurrency level of 10:\n",
    "- We run 100 iterations\n",
    "- Each iteration runs 10concurrent queris\n",
    "- Total queries = 1,000 queries (TOTAL_ITERATIONS*CONCURRENT_QUERIES)\n",
    "\n",
    "NOTE: For each concurrency level in CONCURRENT_QUERIES:\n",
    "1. Run TOTAL_QUERIES iterations\n",
    "2. In each iteration, execute that many cuncurent queries\n",
    "3. Measure and collect latencies for all queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d76935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the performance_guidance_results results dictionary\n",
    "performance_guidance_results = {\"PostgreSQL\": {}, \"MongoDB\": {}}\n",
    "\n",
    "# The concurrency levels for performance guiddance testing\n",
    "# Each level represents the number of simulteneous queries to execute\n",
    "# This is important to note for AI workloads as it will affect the performance of the system as the number of requests increase\n",
    "CONCURRENT_QUERIES = [\n",
    "    1,\n",
    "    2,\n",
    "    4,\n",
    "    5,\n",
    "    8,\n",
    "] # 24, 32, 40, 48, 50, 56, 64, 72, 80, 88, 96, 100, 200, 400\n",
    "\n",
    "# The total number of iterations to run for each concurrency level\n",
    "# Thi sis important to note for AI workloads as it will affect the performance of the system as the number of queries per request increases\n",
    "TOTAL_QUERIES = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2444b9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# Function to securely get and set environment variables\n",
    "def set_env_securely(var_name, prompt):\n",
    "    value = getpass.getpass(prompt)\n",
    "    os.environ[var_name] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13f9b86",
   "metadata": {},
   "source": [
    "### Step 1: Install Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f08f981",
   "metadata": {},
   "source": [
    "All the libraries are installed using pip and facilitate the sourcing of data, embedding generation, and data visualization.\n",
    "- `datasets`: Hugging Face library for managing and preprocessing datasets across text, image and audio (https://huggingface.co/datasets)\n",
    "- `sentence_transformers`: For creating sentence embeddings for tasks like semantic search and clustering (https://www.sbert.net/)\n",
    "- `pandas`: A library for data manipulation and analysis with DataFrames and Series (https://pandas.pydata.org/)\n",
    "- `matplotlib`: A library for creating static, interactive and animated data visualizations (https://matplotlib.org/)\n",
    "- `seaborn`: A library for creating statistical data visualizations (https://seaborn.pydata.org/)\n",
    "- `cohere`: A library for generating embeddings and accessing the Cohere API or models (https://cohere.ai/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4c6a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade datasets sentence_transformers pandas matplotlib seaborn cohere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5d9760",
   "metadata": {},
   "source": [
    "### Step 2: Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a6c52a",
   "metadata": {},
   "source": [
    "The dataset for the notebook is sourced from the Hugging Face Cohere Wikipedia dataset.\n",
    "\n",
    "The [Cohere/wikipedia-22-12-en-embeddings](https://huggingface.co/datasets/Cohere/wikipedia-22-12-en-embeddings) dataset on Hugging Face comprises English Wikipedia articles embedded using Cohere's multilingual-22-12 model. Each entry includes the article's title, text, URL, Wikipedia ID, view count, paragraph ID, language codes, and a 768-dimensional embedding vector. This dataset is valuable for tasks like semantic search, information retrieval and NLP model training.\n",
    "\n",
    "For this notebook we are using 100,000 rows of the dataset and have removed the id, wiki_id, paragraph_id, langs and view columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bed2516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Using 100,000 rows for testing feel free to change this to any number of rows you want to test\n",
    "# The wikipedia-22-12-en-embeddings dataset has approximately 35,000,000 rows and requires 120GB of memory to load\n",
    "MAX_ROWS = 100000\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"Cohere/wikipedia-22-12-en-embeddings\", split=\"train\", streaming=True\n",
    ")\n",
    "dataset_segment = dataset.take(MAX_ROWS)\n",
    "\n",
    "# Convert the dataset to a pandas dataframe\n",
    "dataset_df = pd.DataFrame(dataset_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cac2bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a JSON attribute to the dataset consisting of the title, text and url\n",
    "dataset_df[\"json_data\"] = dataset_df.apply(\n",
    "    lambda row: {\"title\": row[\"title\"], \"text\": row[\"text\"], \"url\": row[\"url\"]}, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f345bd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the id field, wiki_id, paragraph_id, langs and views from the dataset\n",
    "# This is to replicate the structure of dataset usually encountered in AI workloads, particularly particularly in RAG systems where metadata is extracted from documents and stored.\n",
    "dataset_df = dataset_df.drop(\n",
    "    columns=[\"id\", \"wiki_id\", \"paragraph_id\", \"langs\", \"views\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398292ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the emb column name to embedding\n",
    "dataset_df = dataset_df.rename(columns={\"emb\": \"embedding\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6298b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee5ac63",
   "metadata": {},
   "source": [
    "### Step 3: Embedding Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333eb9c6",
   "metadata": {},
   "source": [
    "We use the Cohere API to generate embeddings for the test queries.\n",
    "\n",
    "To get the Cohere API key, you can sign up for a free account on the Cohere website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dd760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Cohere API key\n",
    "set_env_securely(\"COHERE_API_KEY\", \"Enter your Cohere API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07c8f9c",
   "metadata": {},
   "source": [
    "Using the Cohere API to generate embeddings for the test queries.\n",
    "\n",
    "Using the `embed_multilingual-v2.0` model. This is the same model used in the Cohere Wikipedia dataset.\n",
    "\n",
    "Embedding size is 768 dimensions and the precision is float32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9f7ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import cohere\n",
    "\n",
    "# Initialize Cohere Client\n",
    "co = cohere.Client()\n",
    "\n",
    "\n",
    "def get_cohere_embeddings(\n",
    "        sentences: List[str],\n",
    "        model: str = \"embed-multilingual-v2.0\",\n",
    "        input_type: str = \"search_document\",\n",
    ")-> Tuple[List[float], List[int]]:\n",
    "    \"\"\"\n",
    "    Generate embeddings for the provided sentences using Cohere's embedding model.\n",
    "\n",
    "    Args:\n",
    "    sentences (list of str): List of sentences to generate embeddings for.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[List[float], List[int]]: A tuple containing two lists of embeddings (float and int8).\n",
    "    \"\"\"\n",
    "    generated_embedding = co.embed(\n",
    "        texts=sentences,\n",
    "        model=\"embed-multilingual-v2.0\",\n",
    "        input_type=\"search_document\",\n",
    "        embedding_types=[\"float\"],\n",
    "    ).embeddings\n",
    "\n",
    "    return generated_embedding.float[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cac91ca",
   "metadata": {},
   "source": [
    "Generate embeddings for the query templates\n",
    "\n",
    "Store the embeddings in a dictionary for easy access\n",
    "\n",
    "Note: Doing this to avoid tje overhead of generating embeddings for each query in the dataset during the performance analysis process, as this is a time consuming process and expensive in terms of API usage.\n",
    "\n",
    "Note: Feel free to add more queries to the query_templates list to test the performance of the vector database with a larger number of queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a753a18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_templates = [\n",
    "    \"When was YouTube officially launched, and by whom?\",\n",
    "    \"What is YouTube's slogan introduced after Google's acquisition?\",\n",
    "    \"How many hours of videos are collectively watched on YouTube daily?\",\n",
    "    \"Which was the first video uploaded to YouTube, and when was it uploaded?\",\n",
    "    \"What was the acquisition cost of YouTube by Google, and when was the deal finalized?\",\n",
    "    \"What was the first YouTube video to reach one million views, and when did it happen?\",\n",
    "    \"What are the three separate branches of the United States government?\",\n",
    "    \"Which counrty has the highest documented incarceration rate and prison population?\",\n",
    "    \"How many executions have occured in the United States since 1977, and which countries have more?\",\n",
    "    \"What percentage of the global military spending did the United States account for in 2019?\",\n",
    "    \"How is the U.S. president elected?\",\n",
    "    \"What cooling system innovation was included in the proposed venues for the World Cup in Qatar?\",\n",
    "    \"What lawsuit was giled against Google in June 2022, and what was it about?\",\n",
    "    \"How much was Google fined by CNIL in January 2022, and for what reason?\",\n",
    "    \"When did YouTube join the NSA's PRISM program, according to reports?\",    \n",
    "]\n",
    "\n",
    "# For each query template question generate the embedding\n",
    "# NOTE: Doing this to avoid the overhead of generating embeddings for each query during the performance comparison\n",
    "query_embeddings = [\n",
    "    get_cohere_embeddings(sentences=[query], input_type=\"search_query\")\n",
    "    for query in query_templates\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03687dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with the query templates and their corresponding embeddings\n",
    "query_embeddings_dict = {\n",
    "    query: embedding for query, embedding in zip(query_templates, query_embeddings)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd99d649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first 5 query embeddings as a dataframe\n",
    "pd.DataFrame(query_embeddings_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b70c2a5",
   "metadata": {},
   "source": [
    "## Part 2: Retrieval Mechanisms with PostgreSQL and PgVector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b873ffe2",
   "metadata": {},
   "source": [
    "In this section we create a PostgreSQL database with the PgVector extention and insert the dataset into the database.\n",
    "\n",
    "We are also going to implement various search mechanisms on the databases to test the performance of the database under certain search patterns and workloads. Specifically we are going to implement a semantic search mechanism on the database via vector sesarch and hybrid search mechanism on the database via vector search and text search.\n",
    "\n",
    "The table `wikipedia_data` is created with the following columns:\n",
    "- `id`: The unique identifier for each row\n",
    "- `title`: The title of the Wikipedia article \n",
    "- `text`: The text of the Wikipedia article\n",
    "- `url`: The URL of the Wikipedia article\n",
    "- `json_data`: The JSON data of the Wikipedia article\n",
    "- `embedding`: The embedding vector for the Wikipedia article\n",
    "\n",
    "The table is created with HNSW index with m=16, ef_construction=64 and cosine similarity (these are the default parameters for the HNSW index in pgvector).\n",
    "- `HNSW`: Hierarchical Navigable Small World graphs are a type of graph-based index that are used for efficient similarity search.\n",
    "- `m=16`: The number of edges per node in the graph\n",
    "- `ef_construction=64`: Short for exploration factor construction, is the number of edges to build during the index construction phase\n",
    "- `ef_search=100`: Short for exploration factor search, is the number of edges to search during the index search phase\n",
    "- `cosine similarity`: The similarity metric used for the index (formula: dot product(A,B)/(|A||B|))\n",
    "- `cosine distance`: The distance metric calculated using cosine similarity (1 - cosine similarity)\n",
    "\n",
    "We perform a semantic search on the database using a single data point of the query templates and their corresponding embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fc18e1",
   "metadata": {},
   "source": [
    "### Step 1: Install Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f682c19a",
   "metadata": {},
   "source": [
    "- `pgvector` (0.3.6): A PostgreSQL extension for vector similarity search (https://github.com/pgvector/pgvector)\n",
    "- `psycopg` (3.2.3): A PostgreSQL database adapter for Python (https://www.psycopg.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ca5331",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pgvector \"psycopg[binary]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4368b8",
   "metadata": {},
   "source": [
    "### Step 2: Create Posgres Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7126e1ab",
   "metadata": {},
   "source": [
    "- `id`: The unique identifier for each row\n",
    "- `title`: The title of the Wikipedia article \n",
    "- `text`: The text of the Wikipedia article\n",
    "- `url`: The URL of the Wikipedia article\n",
    "- `json_data`: The JSON data of the Wikipedia article\n",
    "- `embedding`: The embedding vector for the Wikipedia article\n",
    "\n",
    "**Key aspect of PostgreSQL table creation:**\n",
    "- `id`: The unique identifier for each row stored with the data type `bigserial` which is a 64-bit integer and auto-incremented.\n",
    "- `title`: The title of the Wikipedia article stored with the data type `text` which is a variable character string.\n",
    "- `text`: The text of the Wikipedia article stored with the data type `text` which is a variable character string.\n",
    "- `url`: The URL of the Wikipedia article stored with the data type `text` which is a variable character string.\n",
    "- `json_data`: The JSON data of the Wikipedia article stored with the data type `jsonb` which is a binary formatted JSON data type.\n",
    "- `embedding`: The embedding vector for the Wikipedia article stored with the data type `vector(768)` which is a provided by pgvector and is of 768 dimentions.\n",
    "\n",
    "**Optimizing the table for search:**\n",
    "- `search vector`: The search vector for the Wikipedia article stored with the data type `tsvector` which is a text search data type in PostgreSQL.\n",
    "- The expression inside the `GENERATED ALWAYS AS` clause is the text (title and text) to be tokenized and indexed for full-text search.\n",
    "- Using `coalesce` to handle any null values in the title or text columns.\n",
    "- `STORED`: The keyword indicates that the `search_vector` column is stored in the table, this avoids the overhead of recalculating the `search_vector` column during queries, and improves performance.\n",
    "\n",
    "**Extra**\n",
    "- The `serach_vector` column is computed automatically using the text in the `title` and `text` fields, making full-text search more efficient by avoiding on-the-fly computation.\n",
    "- The `HNSW` index on the `embedding` column is optimized for ANN queries using cosine similarity, which is crucial for semantic search.\n",
    "- The `GIN` indexes on both the `json_data` and `search_vector` columns ensure fast query performance on JSONB queries and full-text search, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cf69f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(connection):\n",
    "    with connection.cursor() as cur:\n",
    "        # Drop table if it already exists\n",
    "        cur.execute(\"DROP TABLE IF EXISTS wikipedia_data\")\n",
    "\n",
    "        # Create the table with the appropriate structure\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE TABLE wikipedia_data (\n",
    "                id bigserial PRIMARY kEY,\n",
    "                title text,\n",
    "                text text,\n",
    "                url text,\n",
    "                json_data jsonb,\n",
    "                embedding vector(768),\n",
    "                search_vector tsvector GENERATED ALWAYS AS (\n",
    "                    to_tsvector('english', coalesce(title, '') || ' ' || coalesce(text, ''))\n",
    "                ) STORED\n",
    "            )\n",
    "        \"\"\")\n",
    "        # Create HNSW index for vectorsimilarity search with cosine similarity\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE INDEX wikipedia_data_embedding_hnsw_idx\n",
    "            ON wikipedia_data\n",
    "            USING hnsw (embedding_vector_cosine_ops)\n",
    "            WITH (m = 16, ef_construction = 64);\n",
    "        \"\"\")\n",
    "\n",
    "        # Create GIN index on the json_data column for efficient JSONB queruing\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE INDEX wikipedia_data_embedding_gin_idx\n",
    "            ON wikipedia_data\n",
    "            USING GIN (json_data);\n",
    "        \"\"\")\n",
    "\n",
    "        # Create GIN index on the search_vector column for efficient full-text search\n",
    "        cur.execute(\"\"\"\n",
    "            CREATE INDEX wikipedia_data_search_vector_idx\n",
    "            ON wikipedia_data\n",
    "            USING GIN (search_vector);\n",
    "        \"\"\")\n",
    "\n",
    "        print(\"Table and indexes created successfully\")\n",
    "        connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148600d0",
   "metadata": {},
   "source": [
    "### Step 4: Define insert function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd503c3a",
   "metadata": {},
   "source": [
    "For inserting JSON data, we convert the Python Dictionary in the `json_data` attribute to a JSON string using the `json.dumps()` function. \n",
    "\n",
    "This is a serialization process that converts the Python Dictionary in the `json_data` attribute to a JSON string that is stored as binary data in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fcf18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def insert_data_to_postgres(dataframe, connection, database_type=\"PostgreSQL\"):\n",
    "    \"\"\"\n",
    "    Insert data into the PostgreSQL database.\n",
    "\n",
    "    Args:\n",
    "    dataframe (pandas.DataFrame): The dataframe containing the data to insert.\n",
    "    connection (psycopg.extensions.connection): The connection to the PostgreSQL database.\n",
    "    database_type (str): The type of database (default: \"PostgreSQL\"). \n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    total_rows = len(dataframe)\n",
    "\n",
    "    try:\n",
    "        with connection.cursor() as cur:\n",
    "            # Create a list of tuples for insertion, filtering out rows with invalid embeddings\n",
    "            data_tuples = []\n",
    "            for _, row in dataframe.iterrows():\n",
    "                data_tuple = (\n",
    "                    row[\"title\"],\n",
    "                    row[\"text\"],\n",
    "                    row[\"url\"],\n",
    "                    json.dumps(row[\"json_data\"]), # convert dict to JSON string\n",
    "                    row[\"embedding\"],\n",
    "                )\n",
    "                data_tuples.append(data_tuple)\n",
    "\n",
    "            if not data_tuples:\n",
    "                raise ValueError(\"No valid data tuples to insert\")\n",
    "            \n",
    "            cur.executemary(\n",
    "                \"\"\"\n",
    "                INSERT INTO wikipedia_data\n",
    "                (title, text, url, json_data, embedding)\n",
    "                VALUES (%s, %s, %s, %s, %s)\n",
    "                \"\"\",\n",
    "                data_tuples,\n",
    "            )\n",
    "\n",
    "            connection.commit()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erroe during bulk insert: {e}\")\n",
    "        connection.rollback()\n",
    "        raise e\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    rows_per_second = len(data_tuples) / total_time\n",
    "\n",
    "    # print(f\"\\nInsertion Statistics:\")\n",
    "    # print(f\"Total time: {total_time:.2f} seconds\")\n",
    "    # print(f\"Average insertion rate: {rows_per_second:.2f} rows/second\")\n",
    "    # print(f\"Total rows inserted: {len(data_tuples)}\")\n",
    "    # print(f\"Rows skipped: {total_rows - len(data_tuples)}\")\n",
    "\n",
    "    # Store results in performance guidance dictionary\n",
    "    if database_type not in performance_guidance_results:\n",
    "        performance_guidance_results[database_type] = {}\n",
    "\n",
    "    performance_guidance_results[database_type][\"insert_time\"] = {\n",
    "        \"total_time\": total_time,\n",
    "        \"rows_per_second\": rows_per_second,\n",
    "        \"total_rows\": total_rows,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2e334e",
   "metadata": {},
   "outputs": [],
   "source": [
    "neon_db_host = os.environ[\"PGHOST\"]\n",
    "neon_db_database = os.environ[\"PGDATABASE\"]\n",
    "neon_db_user = os.environ[\"PGUSER\"]\n",
    "neon_db_password = os.environ[\"PGPASSWORD\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be4492d",
   "metadata": {},
   "source": [
    "### Step 5: Insert data into Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4798f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg\n",
    "from pgvector.psycopg import register_vector\n",
    "\n",
    "try:\n",
    "    # Connect to PostgreSQL\n",
    "    conn = psycopg.connect(\n",
    "        f\"dbname={neon_db_database} user={neon_db_user} password={neon_db_password} host={neon_db_host}\"\n",
    "    )\n",
    "\n",
    "    # Enable the pgvector extension\n",
    "    conn.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n",
    "\n",
    "    # Register vector type to handle embedding data as vector data types\n",
    "    register_vector(conn)\n",
    "\n",
    "    # Step 1: Create the table\n",
    "    create_table(conn)\n",
    "\n",
    "    # Step 2: Insert the expended dataset into the table\n",
    "    insert_data_to_postgres(dataset_df, conn)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Failed to execute: \", e)\n",
    "finally:\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "    print(\"Connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30829672",
   "metadata": {},
   "source": [
    "### Step 6: Define Text search function with Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5076f104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_search_with_postgres(query, connection, top_n=5):\n",
    "    \"\"\"\n",
    "    Perform a full-text search on the precomputed 'search_vector' column of the 'wikipedia_data' table. \n",
    "    \"\"\"\n",
    "    with connection.cursor() as cur:\n",
    "        # Convert the search query into a tsquery\n",
    "        cur.execute(\"SELECT plainto_tsquery('english', %s)\", (query,))\n",
    "        ts_query = cur.fetchone()[0]\n",
    "\n",
    "        # Execute the full-text search query using the precomputed search_vector column\n",
    "        cur.execute(\n",
    "            \"\"\" \n",
    "            SELECT title, text, url, json_data,\n",
    "                ts_rank_cd(search_vector, %s) AS rank\n",
    "            FROM wikipedia_data\n",
    "            WHERE search_vector @@ %s\n",
    "            ORDER BY rank DESC\n",
    "            LIMIT %s\n",
    "            \"\"\",\n",
    "            (ts_query, ts_query, top_n),\n",
    "        )\n",
    "\n",
    "        results = cur.fetchall()\n",
    "\n",
    "        formatted_results = [\n",
    "            {\n",
    "                \"title\": r[0],\n",
    "                \"text\": r[1],\n",
    "                \"url\": r[2],\n",
    "                \"json_data\": r[3],\n",
    "                \"rank\": r[4],\n",
    "            }\n",
    "            for r in results\n",
    "        ]\n",
    "\n",
    "        return formatted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07ac48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Connect to PostgreSQL\n",
    "    conn = psycopg.connect(\n",
    "        f\"dbname={neon_db_database} user={neon_db_user} password={neon_db_password} host={neon_db_host}\"\n",
    "    )\n",
    "\n",
    "    text_search_with_postgres_results = text_search_with_postgres(\n",
    "        \"When was Youtube officially launched, and by whom?\", conn\n",
    "    )\n",
    "    # Print results in a formatted way\n",
    "    for result in text_search_with_postgres_results:\n",
    "        print(f\"\\nTitle: {result}\")\n",
    "        print(f\"Text: {result['text']}\")\n",
    "        print(f\"URL: {result['url']}\")\n",
    "        print(f\"JSON Data: {result['json_data']}\")\n",
    "        print(f\"Rank: {result['rank']:.4f}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Failed to connect or execute query:\", e)\n",
    "finally:\n",
    "    conn.close()\n",
    "    print(\"Connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1626e3",
   "metadata": {},
   "source": [
    "### Step 7: Define vector search function with Postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262dd42d",
   "metadata": {},
   "source": [
    "To avoid exhasuting API key usage, we will fetch the query embedding from the `query_embeddings_dict` dictionary.\n",
    "\n",
    "In the `vector_search_with_postgres` function, we set the HNSW ef parameter to 100 using the `execute_command` function.\n",
    "\n",
    "This is to set the exploration factor for the HNSW index to 100. And corresponds to the number of nodes/candidates to search during the index search phase. A node corresponds to a vector in the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797f1190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search_with_postgres(\n",
    "        query, connection, top_n=5, filter_key=None, filter_value=None\n",
    "):\n",
    "    # Retrieve the query embedding from the query_embeddings_dict\n",
    "    query_embedding = query_embeddings_dict[query]\n",
    "\n",
    "    with connection.cursor() as cur:\n",
    "        # Set the HNSW ef parameter\n",
    "        cur.execute(\"SET hnsw.ef_search = 100\")\n",
    "        connection.commit()\n",
    "\n",
    "        # Construct the base SQL query\n",
    "        sql_query = \"\"\" \n",
    "            SELECT title, text, url, json_data,\n",
    "                embedding <=> %s::vector AS similarity\n",
    "            FROM wikipedia_data\n",
    "        \"\"\"\n",
    "\n",
    "        # Append the filter condition if provided\n",
    "        if filter_key and filter_value:\n",
    "            sql_query += \" WHERE json_data->>%s = %s\"\n",
    "\n",
    "        # Append the ORDER BY and LIMIT clauses\n",
    "        sql_query += \"\"\" \n",
    "            ORDER BY similarity ASC\n",
    "            LIMIT %s\n",
    "        \"\"\"\n",
    "\n",
    "        # Prepare the parameters for the query\n",
    "        params = [query_embedding]\n",
    "        if filter_key and filter_value:\n",
    "            params.extend([filter_key, filter_value])\n",
    "        params.append(top_n)\n",
    "\n",
    "        # Execute the query with parameters\n",
    "        cur.execute(sql_query, params)\n",
    "\n",
    "        # Fetch and return the top results\n",
    "        results = cur.fetchall()\n",
    "\n",
    "        # Format results as a list of dictionaries for easier handling\n",
    "        formatted_results = [\n",
    "            {\n",
    "                \"title\": r[0],\n",
    "                \"text\": r[1],\n",
    "                \"url\": r[2],\n",
    "                \"json_data\": r[3],\n",
    "                \"similarity\": r[4],\n",
    "            }\n",
    "            for r in results\n",
    "        ]\n",
    "\n",
    "        return formatted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dc5bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to PostgreSQL\n",
    "try:\n",
    "    conn = psycopg.connect(\n",
    "        f\"dbname={neon_db_database} user={neon_db_user} password={neon_db_password} host={neon_db_host}\"\n",
    "    )\n",
    "\n",
    "    # Run semantic search with a sample query\n",
    "    query_text = \"When YouTube officially launched, and by whom?\"\n",
    "    results = vector_search_with_postgres(\n",
    "        query_text, conn, top_n=5, filter_key=\"title\", filter_value=\"YouTube\"\n",
    "    )\n",
    "\n",
    "    # Print results in a formatted way\n",
    "    for result in results:\n",
    "        print(f\"\\nTitle: {result['title']}\")\n",
    "        print(f\"Text: {result['text']}\")\n",
    "        print(f\"URL: {result['url']}\")\n",
    "        print(f\"JSON Data: {result['json_data']}\")\n",
    "        print(f\"Similarity Score: {1- result['similarity']:.4f}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Failed to connect or execute query:\", e)\n",
    "finally:\n",
    "    conn.close()\n",
    "    print(\"Connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f69306",
   "metadata": {},
   "source": [
    "### Step 8: Define hybrid search function with Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9480b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from psycopg import sql\n",
    "\n",
    "\n",
    "def hybrid_search_with_postgres(\n",
    "        query, connection, top_n=5, filter_key=None, filter_value=None\n",
    "):\n",
    "    \"\"\" \n",
    "    Perform a hybrid search combining semantic vector similarity and full-text search.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "        connection: A psycopg2 database connction object.\n",
    "        top_n (int): Number of top results to return (default is 5).\n",
    "        filter_key (str, optional): JSON key to filter rusults on.\n",
    "        filter_value (str, optional): Value of the JSON key to filter results.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing the search results.\n",
    "    \"\"\"\n",
    "    # Retrieve the query embedding from the query_embeddings_dict\n",
    "    query_embedding = query_embeddings_dict[query]\n",
    "\n",
    "    with connection.cursor() as cur:\n",
    "        # Set the HNSW ef_search parameter for vecotr search \n",
    "        cur.execute(\"SET ivfflat.probes = 10\")\n",
    "        connection.commit()\n",
    "\n",
    "        # Base SQL components\n",
    "        base_vector_query = sql.SQL(\"\"\"\n",
    "            SELECT id, title, text, url, json_data,\n",
    "                    embedding <=> %s::vector AS vector_similarity\n",
    "            FROM wikipedia_data\n",
    "        \"\"\")\n",
    "        base_full_text_query = sql.SQL(\"\"\"\n",
    "            SELECT id, title, text, url, json_data,\n",
    "                    ts_rank_cd(search_vector, plainto_tsquery('english', %s)) AS text_rank\n",
    "            FROM wikipedia_data\n",
    "            WHERE search_vector @@ plainto_tsquery('english', %s)\n",
    "        \"\"\")\n",
    "\n",
    "        # Initialize parameters list\n",
    "        vector_params = [query_embedding]\n",
    "        text_params = [query, query]\n",
    "\n",
    "        # Append filter condition if provided\n",
    "        if filter_key and filter_value:\n",
    "            filter_condition = sql.SQL(\"json_data->>{} = %s\").format(\n",
    "                sql.Literal(filter_key)\n",
    "            )\n",
    "            base_vector_query += sql.SQL(\" WHERE \") + filter_condition\n",
    "            base_full_text_query += sql.SQL(\" AND \") + filter_condition\n",
    "            vector_params.append(filter_value)\n",
    "            text_params.append(filter_value)\n",
    "\n",
    "        # Execute the vector similarity search \n",
    "        cur.execute(base_vector_query + sql.SQL(\" LIMIT %s\"), vector_params + [top_n])\n",
    "        vector_results = cur.fetchall()\n",
    "\n",
    "        # Execute the full-text search\n",
    "        cur.execute(base_full_text_query + sql.SQL(\" LIMIT %s\"), text_params + [top_n])\n",
    "        text_results = cur.fetchall()\n",
    "\n",
    "        # Combine and rank results using Reciprocal Rank Fusion (RRF)\n",
    "        combined_results = {}\n",
    "        rrf_k = 60 # RRF parameter; adjust as needed\n",
    "\n",
    "        # Process vector results\n",
    "        for rank, row in enumerate(vector_results, start=1):\n",
    "            doc_id = row[0]\n",
    "            if doc_id not in combined_results:\n",
    "                combined_results[doc_id] = {\n",
    "                    \"title\": row[1],\n",
    "                    \"text\": row[2],\n",
    "                    \"url\": row[3],\n",
    "                    \"json_data\": row[4],\n",
    "                    \"vector_similarity\": row[5],\n",
    "                    \"text_rank\": 0,\n",
    "                    \"rrf_score\": 0,\n",
    "                }\n",
    "            combined_results[doc_id][\"rrf_score\"] += 1 / (rrf_k + rank)\n",
    "\n",
    "        # Process full_text results\n",
    "        for rank, row in enumerate(text_results, start=1):\n",
    "            doc_id = row[0]\n",
    "            if doc_id not in combined_results:\n",
    "                combined_results[doc_id] = {\n",
    "                    \"title\": row[1],\n",
    "                    \"text\": row[2],\n",
    "                    \"url\": row[3],\n",
    "                    \"json_data\": row[4],\n",
    "                    \"vector_similarity\": None,\n",
    "                    \"text_rank\": row[5],\n",
    "                    \"rrf_score\": 0,\n",
    "                }\n",
    "            combined_results[doc_id][\"rrf_score\"] += 1 / (rrf_k + rank)\n",
    "\n",
    "        # Sort combined results by RRF score in descending order\n",
    "        sorted_results = sorted(\n",
    "            combined_results.values(), key=lambda x: x[\"rrf_score\"], reverse=True\n",
    "        )\n",
    "\n",
    "        # Return the top_n results\n",
    "        return sorted_results[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541be6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Connect to PostgreSQL\n",
    "    conn = psycopg.connect(\n",
    "        f\"dbname={neon_db_database} user={neon_db_user} password={neon_db_password} host={neon_db_host}\"\n",
    "    )\n",
    "\n",
    "    # Run hybrid search with a sample query\n",
    "    query_text = \"When was YouTube officially launched, and by whom?\"\n",
    "    results = hybrid_search_with_postgres(\n",
    "        query_text, conn, top_n=5, filter_key=\"title\", filter_value=\"YouTube\"\n",
    "    )\n",
    "    for result in results:\n",
    "        print(f\"\\nTitle: {result['title']}\")\n",
    "        print(f\"Text: {result['text']}\")\n",
    "        print(f\"URL: {result['url']}\")\n",
    "        print(f\"JSON Data: {result['json_data']}\")\n",
    "        if result[\"vector_similarity\"] is not None:\n",
    "            print(f\"Vector Similarity Score: {1 - result['vector_similarity']:.4f}\")\n",
    "        if result[\"text_rank\"] > 0:\n",
    "            print(f\"Text Rank: {result['text_rank']:.4f}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Failed to connect or execute query: \", e)\n",
    "finally:\n",
    "    conn.close()\n",
    "    print(\"Connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5d6005",
   "metadata": {},
   "source": [
    "## Part 3: Retrieval Mechanisms with MongoDB Atlas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37870fea",
   "metadata": {},
   "source": [
    "### Step 1: Install Libraries\n",
    "\n",
    "- `pymongo` (4.10.1): A pyton driver for MongoDB (https://pymongo.readthedocs.io/en/stable/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3094a427",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade pymongo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630d4fb1",
   "metadata": {},
   "source": [
    "### Step 2: Create MongoDB Atlas Account\n",
    "\n",
    "TODO: Place infoiant required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655af09a",
   "metadata": {},
   "source": [
    "### Step 3: Connect to MongoDB and Create Database and Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b251b3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set MongoDB URI\n",
    "# Example: mongodb+srv://<db_username>:<db_password>@cluster0.wi4s1.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\n",
    "set_env_securely(\"MONGO_URI\", \"Enter your MONGO URI: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30502155",
   "metadata": {},
   "source": [
    "In the following code blocks below we do the following:\n",
    "\n",
    "1. Establish a connection to the MongoDB database\n",
    "2. Create a database and connection if they do not already exist\n",
    "3. Delete all data in the collection if it already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650e4523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "def get_mongo_client(mongo_uri):\n",
    "    \"\"\"Establish and validate connection to the MongoDB.\"\"\"\n",
    "\n",
    "    client = pymongo.MongoClient(\n",
    "        mongo_uri, appname=\"devrel.showcase.postgres_neon_vs_mongodb_atlas.python\"\n",
    "    )\n",
    "\n",
    "    # Validate the connection\n",
    "    ping_result = client.admin.command(\"ping\")\n",
    "    if ping_result.get(\"ok\") == 1.0:\n",
    "        # Connection successful\n",
    "        print(\"Connection to MongoDB successful\")\n",
    "        return client\n",
    "    else:\n",
    "        print(\"Connection to MongoDB failed\")\n",
    "    return None\n",
    "\n",
    "\n",
    "MONGO_URI = os.environ[\"MONGO_URI\"]\n",
    "if not MONGO_URI:\n",
    "    print(\"MONGO_URI not set in environment variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd35ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo.errors import CollectionInvalid\n",
    "\n",
    "mongo_client = get_mongo_client(MONGO_URI)\n",
    "\n",
    "DB_NAME = \"vector_db\"\n",
    "COLLECTION_NAME = \"wikipedia_data\"\n",
    "\n",
    "# Create or get the database\n",
    "db = mongo_client[DB_NAME]\n",
    "\n",
    "# Check if the collection exists\n",
    "if COLLECTION_NAME not in db.list_collection_names():\n",
    "    try: # Create the collection\n",
    "        db.create_collection(COLLECTION_NAME)\n",
    "        print(f\"Collection '{COLLECTION_NAME}' created successfully.\")\n",
    "    except CollectionInvalid as e:\n",
    "        print(f\"Error creating collection: {e}\")\n",
    "else:\n",
    "    print(f\"Collection '{COLLECTION_NAME}' already exists.\")\n",
    "\n",
    "# Assign the collection\n",
    "collection = db[COLLECTION_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2b5280",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.delete_many({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eef367",
   "metadata": {},
   "source": [
    "### Step 4: Vector Index Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e746e3e",
   "metadata": {},
   "source": [
    "The `setup_vecto_search` function creates a vector search index for the MongoDB collection.\n",
    "\n",
    "The `index_name` parameter is the name of the index to create.\n",
    "\n",
    "The `embedding_field_name` parameter is the name of the field containing the text embeddings on each document within the wikipedia_data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c453d655",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_field_name = \"embedding\"\n",
    "vector_search_index_name = \"vector_index\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b867afa6",
   "metadata": {},
   "source": [
    "Filtering your data is useful to narrow the scope of your semantic search and ensure that not all vectors are considered for comparison. It reduces the number of documents against which to run similarity comparisons, which can decrease query latency and increase the accuracy of search results.\n",
    "\n",
    "You must index the fields that you want to filter by using the filter type inside the fields array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5f0a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pymongo.operations import SearchIndexModel\n",
    "\n",
    "def setup_vector_search_index(collection, index_name=\"vector_index\"):\n",
    "    \"\"\"\n",
    "    Setup a vector search index for MongoDB collection.\n",
    "\n",
    "    Args:\n",
    "    collection: MongoDB collection object\n",
    "    index_definition: Dictionary containing the index definition\n",
    "    index_name: Name of the index (default: \"vector_index\") \n",
    "    \"\"\"\n",
    "    new_vector_search_index_model = SearchIndexModel(\n",
    "        definition={\n",
    "            \"fields\": [\n",
    "                {\n",
    "                    \"type\": \"vector\",\n",
    "                    \"path\": \"embedding\",\n",
    "                    \"numDimentions\": 768,\n",
    "                    \"similarity\": \"cosine\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"filter\",\n",
    "                    \"path\": \"json_data_title\",\n",
    "                },\n",
    "            ]\n",
    "        },\n",
    "        name=index_name,\n",
    "        type=\"vectorSearch\",\n",
    "    )\n",
    "\n",
    "    # Create the new index\n",
    "    try:\n",
    "        result = collection.create_search_index(model=new_vector_search_index_model)\n",
    "        print(f\"Creating index '{index_name}'...\")\n",
    "\n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating new vector search index '{index_name}': {e!s}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b603cf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_vector_search_index(collection, \"vector_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184814a3",
   "metadata": {},
   "source": [
    "An Atlas Search index is a data structure that categorizes data in an easily searchable format. It is a mapping between terms and the documents that contain those terms. Atlas Search indexes enable faster retrieval of documents using certain identifiers. You must configure an Atlas Search index to query data in your Atlas cluster using Atlas Search.\n",
    "\n",
    "You can create an Atlas Search index on a single field or on multiple fields. We recommend that you index the fields that you regularly use to sort or filter your data in order to quickly retrieve the documents that contain the retrieval data at query-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f321c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_text_search_index(collection, index_name=\"tetx_search_index\"):\n",
    "    \"\"\"\n",
    "    Setup a text search index for a MongoDB collection in Atlas.\n",
    "\n",
    "    Args:\n",
    "        uri (str): MongoDB Atlas connection string.\n",
    "        database_name (str): Name of the database.\n",
    "        collection_name (str): Name of the collection.\n",
    "        index_name (str): Name of the index (default: \"text_search_index\")\n",
    "    \"\"\"\n",
    "    # Define the search index model\n",
    "    search_index_model = SearchIndexModel(\n",
    "        definition={\n",
    "            \"mappings\": {\n",
    "                \"dynamic\": True # Index all fields dynamically\n",
    "            },\n",
    "        },\n",
    "        name=index_name,\n",
    "        type=\"search\",\n",
    "    )\n",
    "\n",
    "    # Create the search index\n",
    "    try:\n",
    "        result = collection.create_search_index(model=search_index_model)\n",
    "        print(f\"Creating index '{index_name}'...\")\n",
    "\n",
    "        # Wait for the index to be created\n",
    "        time.sleep(30)\n",
    "        print(f\"30-second wait completed for index '{index_name}'.\")\n",
    "\n",
    "        print(f\"Index '{index_name}' created successfully.\")\n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating text search index '{index_name}': {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609d18eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_text_search_index(collection, \"text_search_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea428aa",
   "metadata": {},
   "source": [
    "### Step 5: Define Insert Data Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0878bf4",
   "metadata": {},
   "source": [
    "Because of the affinity of MongoDB for JSON data, we don't have to convert the Python Dictionary in the `json_data` attribute to a JSON string using the `json.dumps()` function. Instead, we can directly insert the Python Dictionary into the MongoDB collection.\n",
    "\n",
    "This reduced the operational overhead of the insertion processes in AI workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4395d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data_to_mongodb(dataframe, collection, database_type=\"MongoDB\"):\n",
    "    start_time = time.time()\n",
    "    total_rows = len(dataframe)\n",
    "\n",
    "    try:\n",
    "        # Convert DataFrame to list of dictionaries for MongoDB insertion\n",
    "        documents = dataframe.to_dict(\"records\")\n",
    "\n",
    "        # Use insert_many for better performance\n",
    "        result = collection.insert_many(documents)\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        rows_per_second = total_rows / total_time\n",
    "\n",
    "        # print(f\"\\nMongoDB Insertion Statistics:\")\n",
    "        # print(f\"Total time: {total_time:.2f} seconds\")\n",
    "        # print(f\"Average insertion rate: {rows_per_second:.2f} rows/second\")\n",
    "        # print(f\"Total rows inserted: {len(result.inserted_ids)}\")\n",
    "\n",
    "        # Store results in performance guidance dictionary\n",
    "        if database_type not in performance_guidance_results:\n",
    "            performance_guidance_results[database_type] = {}\n",
    "\n",
    "        performance_guidance_results[database_type][\"insert_time\"] = {\n",
    "            \"total_time\": total_time,\n",
    "            \"rows_per_second\": rows_per_second,\n",
    "            \"total_rows\": total_rows,\n",
    "        }\n",
    "\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during MongoDB insertion: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d6bdca",
   "metadata": {},
   "source": [
    "### Step 6: Insert Data into MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936f7d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = dataset_df.to_dict(\"records\")\n",
    "success = insert_data_to_mongodb(dataset_df, collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086d37e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(performance_guidance_results[\"MongoDB\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83fef7e",
   "metadata": {},
   "source": [
    "### Step 7: Define Text Search Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dfac7b",
   "metadata": {},
   "source": [
    "The `text_search_with_mongodb` function performs a text search in the MongoDB collection based on the user query.\n",
    "\n",
    "- `query_text` parameter is the user's query string.\n",
    "- `collection` parameter is the MongoDB collection to search.\n",
    "- `top_n` parameter is the number of top results to return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa7ac7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_search_with_mongodb(query_text, collection, top_n=5):\n",
    "    \"\"\"\n",
    "    Perform a text search in the MongoDB collection based on the user query.\n",
    "\n",
    "    Args:\n",
    "        query_text (str): The user's query string.\n",
    "        collection (MongoCollection): Th eMongoDB collection to search.\n",
    "        top_n (int): The number of top results to return.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of matching documents. \n",
    "    \"\"\"\n",
    "    # Define the text search stage\n",
    "    # The text operator performs a full-text search using the analyzer that you specify in the index configuration.\n",
    "    # The text operator below uses the default standard analyzer.\n",
    "    text_search_stage = {\n",
    "        \"$search\": {\n",
    "            \"index\": \"text_search_index\",\n",
    "            # Search for the query text in the title field\n",
    "            \"text\": {\"query\": query_text, \"path\": \"title\"},\n",
    "        }\n",
    "    }\n",
    "\n",
    "    limit_stage = {\"$limit\": top_n}\n",
    "\n",
    "    project_stage = {\n",
    "        \"$project\": {\n",
    "            \"id\": 0,\n",
    "            \"title\": 1,\n",
    "            \"text\": 1,\n",
    "            \"url\": 1,\n",
    "            \"json_data\": 1,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Define the aggregate pipeline with the text search stage\n",
    "    pipeline = [text_search_stage, limit_stage, project_stage]\n",
    "\n",
    "    # Execute the search\n",
    "    results = collection.aggregate(pipeline)\n",
    "    return list(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53497988",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"When was YouTube officially launched, and by whom?\"\n",
    "\n",
    "get_knowledge = text_search_with_mongodb(query_text, collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243795b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(get_knowledge).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91199186",
   "metadata": {},
   "source": [
    "### Step 8: Define Vector Search Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbc778c",
   "metadata": {},
   "source": [
    "The `semantic_search_with_mongodb` function performs a vector search in the MongoDB collection based on the user query.\n",
    "\n",
    "- `user_query` parameter is the user's query string\n",
    "- `collection` parameter is the MongoDb collection to search.\n",
    "- `top_n` parameter is the number of top results to return.\n",
    "- `vector_search_index_name` parameter is the name of the vector search index to use for the search.\n",
    "\n",
    "The `numCandidates` parameter is the number of candidate matches to consider. This is set to 100 to match the number of candidate matches to consider in the PostgreSQL vector search.\n",
    "\n",
    "Another point to note is the queries in MongoDB are performed using the `aggregate` function enabled by the MongoDB Query Languege(MQL).\n",
    "\n",
    "This allows for more flexibility in the queries and the ability to perform more complex searches. And data processing operations can be defined as stages in the pipeline. If you are a data engineer, data scientist or ML Engineer, the concept of pipeline processing is a key concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec2a83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search_with_mongodb(\n",
    "        user_query,\n",
    "        collection,\n",
    "        top_k=5,\n",
    "        vector_search_index_name=\"vector_index\",\n",
    "        title_filter=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform a vector search in the MongoDB collection based on user query.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's query string.\n",
    "        collection (MongoCollection): The MongoDB collection to search.\n",
    "        additional_stages (list): Additional aggregation stages to include in the pipeline.\n",
    "        vector_search_index_name (str): The name of the vector search index.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of matching documents. \n",
    "    \"\"\"\n",
    "\n",
    "    # Take a query embedding from the query_embedding_dict\n",
    "    query_embedding = query_embeddings_dict[user_query]\n",
    "\n",
    "    if query_embedding is None:\n",
    "        return \"Invalid query or embedding generation failed.\"\n",
    "    \n",
    "    # Define the vector search stage\n",
    "    vector_search_stage = {\n",
    "        \"$vectorSearch\": {\n",
    "            \"index\": vector_search_index_name, # specifies the index to use for the search\n",
    "            \"queryVector\": query_embedding, # the vector representing the query\n",
    "            \"path\": \"embedding\", # field in the documents containing the vectors to search against\n",
    "            \"filter\": {\"json_data.title\": title_filter},\n",
    "            \"numCandidates\": 100, # number of candidate matches to consider\n",
    "            \"limit\": top_k, # return to top matches \n",
    "        }\n",
    "    }\n",
    "\n",
    "    project_stage = {\n",
    "        \"$project\": {\n",
    "            \"_id\": 0, # Exclude the _id field\n",
    "            \"title\": 1,\n",
    "            \"text\": 1,\n",
    "            \"url\": 1,\n",
    "            \"json_data\": 1,\n",
    "            \"score\": {\n",
    "                \"$meta\": \"vectorSearchScore\" # Include the search score\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Define the aggregate pipeline with the vector search stage\n",
    "    pipeline = [vector_search_stage, project_stage]\n",
    "\n",
    "    # Execute the search\n",
    "    results = collection.aggregate(pipeline)\n",
    "    return list(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732fbd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"When was YouTube officially launched, and by whom?\"\n",
    "\n",
    "get_knowledge = vector_search_with_mongodb(\n",
    "    query_text, collection, title_filter=\"YouTube\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dde0dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(get_knowledge).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bab7baf",
   "metadata": {},
   "source": [
    "### Step 9: Define Hybrid Search Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bad3b7",
   "metadata": {},
   "source": [
    "The `hybrid_search_with_mongodb` function conducts a hybrid search on a MongoDB Atlas collection that combines a vector search and a full-text search using Atlas Search.\n",
    "\n",
    "In the MongoDB hybrid search function, there are two weights:\n",
    "\n",
    "- vector_weight = 0.5: This weight scales the score obtained from the vector search portion.\n",
    "- full_text_weight = 0.5: This weight scales the score from the full-text search portion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f1da8e",
   "metadata": {},
   "source": [
    "#### Note: In the MongoDB hybrid search function, two weights:\n",
    "\n",
    "-  `vector_weight`\n",
    "- `full_text_weight`\n",
    "\n",
    "They are used to control the influence of each search component on the final score.\n",
    "\n",
    "Here's how they work:\n",
    "\n",
    "Purpose: The weights allow you to adjust how much the vector (semantic) search and the full-text search contribute to the overall ranking. For example a higher full_text_weight means that the full-text search results will have a larger impact on the final score, whereas a higher vector_weight would give more importance to the vector similarity score.\n",
    "\n",
    "Usage in the pipeline: Within the aggregation pipeline, after retrieving results from each search type, the function computes a reciprocal ranking score for each result (using an expression like `1/(rank + 60)`). This score is then multiplied by the corresponding weight:\n",
    "\n",
    "**Vector Search:**\n",
    "\n",
    "```\n",
    "\"vs_score\": {\n",
    "    \"$multiply\": [ vector_weight, { \"$devide\": [1.0, { \"$add\": [\"$rank\", 60] } ] } ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Full-Text Search:**\n",
    "```\n",
    "\"fts_score\": {\n",
    "    \"$multiply\": [ full_text_weight, { \"$divide\": [1.0, { \"$add\": [\"$rank\", 60] } ] } ]\n",
    "}\n",
    "```\n",
    "\n",
    "Finally, these weighted scores are combined (typically by adding them together) to produce a final score that determines the ranking of the documents.\n",
    "\n",
    "**Impact:**\n",
    "By adjusting these weights, you can fine-tune the search results to better match your application's needs. For instance, if the full-text component is more reliable for your dataset, you might set full_text_weight higher than vector_weight.\n",
    "\n",
    "The weights in the MongoDB function allow you to balance the contributions from vector-based search components, ensuring that the final ranking score reflects the desired importance of each search method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dea1094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search_with_mongodb(\n",
    "    user_query,\n",
    "    collection,\n",
    "    vector_search_index_name=\"vector_index\",\n",
    "    text_search_index_name=\"text_search_index\",\n",
    "    vector_weight=0.5,\n",
    "    full_text_weight=0.5,\n",
    "    top_k=10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Conduct a hybrid search on a MongoDB Atlas collection that combines a vector search and a full text search using Atlas Search.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's query string.\n",
    "        collection (MongoDBCollection): The MongoDB collection to search.\n",
    "        vector_search_index_name (str): The name of the vector search index.\n",
    "        text_search_index_name (str): The name of the text search index.\n",
    "        vector_weight (float): The weight of the vector search.\n",
    "        full_text_weight (float): The weight of the full-text search.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of documents (dict) with combined scores.\n",
    "    \"\"\"\n",
    "\n",
    "    collection_name = \"wikipedis_data\"\n",
    "    query_vector = query_embedding_dict[user_query]\n",
    "\n",
    "    pipeline = [\n",
    "        {\n",
    "            \"$vectorSearch\": {\n",
    "                \"index\": vector_search_index_name,\n",
    "                \"path\": \"embedding\",\n",
    "                \"queryVector\": query_vector,\n",
    "                \"numCandidates\": 100,\n",
    "                \"limit\": top_k,\n",
    "            }\n",
    "        },\n",
    "        {\"$group\": {\"_id\": None, \"docs\": {\"$push\": \"$$ROOT\"}}},\n",
    "        {\"$unwind\": {\"path\": \"$docs\", \"includeArrayIndex\": \"rank\"}},\n",
    "        {\n",
    "            \"$addFields\": {\n",
    "                \"vs_score\": {\n",
    "                    \"$multiply\": [\n",
    "                        vector_weight,\n",
    "                        {\"$divide\": [0.1, {\"$add\": [\"$rank\", 60]}]},\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\"$project\": {\"vs_score\": 1, \"_id\": \"$docs._id\", \"title\": \"$docs.title\"}},\n",
    "        {\n",
    "            \"$unionWith\": {\n",
    "                \"coll\": collection_name,\n",
    "                \"pipeline\": [\n",
    "                    {\n",
    "                        \"$search\": {\n",
    "                            \"index\": text_search_index_name,\n",
    "                            \"text\": {\"query\": user_query, \"path\": \"title\"},\n",
    "                        }\n",
    "                    },\n",
    "                    {\"$limit\": top_k},\n",
    "                    {\"$group\": {\"_id\": None, \"docs\": {\"$push\": \"$$ROOT\"}}},\n",
    "                    {\"$unwind\": {\"path\": \"$docs\", \"includeArrayIndex\": \"rank\"}},\n",
    "                    {\n",
    "                        \"$addFields\": {\n",
    "                            \"fts_score\": {\n",
    "                                \"$multiply\": [\n",
    "                                    full_text_weight,\n",
    "                                    {\"$divide\": [1.0, {\"$add\": [\"$rank\", 60]}]},\n",
    "                                ]\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"$project\": {\n",
    "                            \"fts_score\": 1,\n",
    "                            \"_id\": \"$docs._id\",\n",
    "                            \"title\": \"$docs.title\",\n",
    "                        }\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$group\": {\n",
    "                \"_id\": \"$_id\",\n",
    "                \"title\": {\"$first\": \"$title\"},\n",
    "                \"vs_score\": {\"$max\": \"$vs_score\"},\n",
    "                \"fts_score\": {\"$max\": \"$fts_score\"},\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$project\": {\n",
    "                \"_id\": 1,\n",
    "                \"title\": 1,\n",
    "                \"vs_score\": {\"$ifNull\": [\"$vs_score\", 0]},\n",
    "                \"fts_score\": {\"$ifNull\": [\"$fts_score\", 0]},\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$project\": {\n",
    "                \"score\": {\"$add\": [\"$fts_score\", \"$vs_score\"]},\n",
    "                \"_id\": 1,\n",
    "                \"title\": 1,\n",
    "                \"url\": 1,\n",
    "                \"text\": 1,\n",
    "                \"json_data\": 1,\n",
    "                \"vs_score\": 1,\n",
    "                \"fts_score\": 1,\n",
    "            }\n",
    "        },\n",
    "        {\"$sort\": {\"score\": -1}},\n",
    "        {\"$limit\": top_k},\n",
    "    ]\n",
    "\n",
    "    results = list(collection.aggregate(pipeline))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791c29ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"When was YouTube officially launched, and by whom?\"\n",
    "\n",
    "results = hybrid_search_with_mongoDB(\n",
    "    query_text, collection, vector_weight=0.1, full_text_weight=0.9, top_k=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be84e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5be5de",
   "metadata": {},
   "source": [
    "## Part 4: Vector Database Performance Analysis Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b35f2b6",
   "metadata": {},
   "source": [
    "### Insertion Performance Analysis Process\n",
    "\n",
    "We are inserting data incrementally with doubling batch sizes and record performance metrics. Notably, we will be measuring the time it takes to insert data inrementally and the number of rows inserted per second.\n",
    "\n",
    "We are using the `insert_data_incrementally` function to insert data incrementally.\n",
    "\n",
    "It starts with a batch size of 1 and doubles the batch size until it has inserted all the data , recording the time it takes to insert the data and the number of rows inserted per second.\n",
    "\n",
    "The key component we are inserted in is the time it takes to insert the data and the number of rows inserted per second. In AI Workloads, there are data ingestion processes that are performed in batches from various data sources. So, in practice, we are interested in the time it takes to insert the data and the number of rows inserted per second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04562840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def insert_data_incrementally(dataframe, connection, daatbase_type=\"PostgreSQL\"):\n",
    "    \"\"\"\n",
    "    Insert data incrementally with doubling batch sizes and record performance metrics. \n",
    "    \"\"\"\n",
    "    incremental_metrics = {}\n",
    "    total_rows = len(dataframe)\n",
    "    remaining_rows = total_rows\n",
    "    start_idx = 0\n",
    "\n",
    "    # Define batch sizes (1, 10, then doubling)\n",
    "    batch_sizes = [1, 10]\n",
    "    current_size = 20\n",
    "    while current_size < total_rows:\n",
    "        batch_sizes.append(current_size)\n",
    "        current_size *= 2\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "        # Skip if we've already inserted all data\n",
    "        if remaining_rows <= 0:\n",
    "            break\n",
    "\n",
    "        # Calculate actual batch size based on remaining rows\n",
    "        actual_batch_size = min(batch_size, remaining_rows)\n",
    "        end_idx = start_idx + actual_batch_size\n",
    "\n",
    "        # Get the batch of data\n",
    "        batch_df = dataframe.iloc[start_idx:end_idx]\n",
    "\n",
    "        # Record start time\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            # Insert data using existing function\n",
    "            if database_type == \"PostgreSQL\":\n",
    "                insert_data_to_postgres(batch_df, connection, database_type)\n",
    "            else: # MongoDB\n",
    "                insert_data_to_mongodb(batch_df, connection, database_type)\n",
    "\n",
    "            # Record end time and calculate metrics\n",
    "            end_time = time.time()\n",
    "            total_time = end_time - start_time\n",
    "            rows_per_seconds = actual_batch_size / total_time\n",
    "\n",
    "            # Store metrics\n",
    "            incremental_metrics[actual_batch_size] = {\n",
    "                \"total_time\": total_time,\n",
    "                \"rows_per_second\": rows_per_second,\n",
    "                \"batch_size\": actual_batch_size,\n",
    "            }\n",
    "\n",
    "            # print(f\"\\nBatch Size {batch_size} Statistics:\")\n",
    "            # print(f\"Total time: {total_time:.2f} seconds\")\n",
    "            # print(f\"Average insertion rate: {rows_per_second:.2f} rows/second\")\n",
    "            # print(f\"Actual rows inserted: {actual_batch_size}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during batch insertion (size {batch_size}): {e}\")\n",
    "            raise e\n",
    "\n",
    "        # Update counters\n",
    "        atart_idx = end_idx\n",
    "        remaining_rows -= actual_batch_size\n",
    "\n",
    "    # Store results in performance guidance dictionary\n",
    "    if database_type not in permormance_guidance_results:\n",
    "        performance_guidance_results[database_type] = {}\n",
    "\n",
    "    performance_guidance_results[database_type][\"incremental_insert\"] = (incremental_metrics)\n",
    "\n",
    "    return incremental_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b8ea2f",
   "metadata": {},
   "source": [
    "#### 1.1 PosgreSQL Insertion Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61ac8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg\n",
    "from pgvector.psycopg import register_vector\n",
    "\n",
    "try:\n",
    "    conn = psycopg.connect(\n",
    "        f\"dbname={neon_db_database} user={neon_db_user} password={neon_db_password} host={neon_db_host}\"\n",
    "    )\n",
    "    register_vector(conn)\n",
    "\n",
    "    # Create fresh table\n",
    "    create_table(conn)\n",
    "\n",
    "    postgres_metrics = insert_data_incrementally(dataset_df, conn, \"PostgreSQL\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Failed to execute: \", e)\n",
    "finally:\n",
    "    conn.close()\n",
    "    print(\"\\nConnection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecfb761",
   "metadata": {},
   "source": [
    "#### 1.2 MongoDB Insertion Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5668d63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    mongo_client = get_mongo_client(MONGO_URI)\n",
    "    db = mongo_client[DB_NAME]\n",
    "    collection = db[COLLECTION_NAME]\n",
    "\n",
    "    # Clear collection\n",
    "    collection.delete_many({})\n",
    "\n",
    "    mongo_metrics = insert_data_incrementally(dataset_df, collection, \"MongoDB\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"MongoDB operation failed: \", e)\n",
    "finally:\n",
    "    mongo_client.close()\n",
    "    print(\"\\nMongoDB connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d16b9d",
   "metadata": {},
   "source": [
    "#### 1.3 Visualize Insertion Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4efd249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_combined_insertion_metrics(postgres_metrics, mongo_metrics):\n",
    "    \"\"\"\n",
    "    Creates a combined line plot compering PostgreSQL and MongoDB insertion metrics. \n",
    "    \"\"\"\n",
    "    # Create figure\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Extract data for both databases\n",
    "    batch_sizes = [\n",
    "        1,\n",
    "        10,\n",
    "        20,\n",
    "        40,\n",
    "        80,\n",
    "        160,\n",
    "        320,\n",
    "        640,\n",
    "        1280,\n",
    "        2560,\n",
    "        5120,\n",
    "        10240,\n",
    "        20480,\n",
    "        40960,\n",
    "    ]\n",
    "    postgers_times = [\n",
    "        postgres_metrics[size][\"total_time\"]\n",
    "        for size in batch_sizes\n",
    "        if size in postgres_metrics\n",
    "    ]\n",
    "    mongo_times = [\n",
    "        mongo_metrics[size][\"total_time\"]\n",
    "        for size in batch_sizes\n",
    "        if size in mongo_metrics\n",
    "    ]\n",
    "\n",
    "    # Create the line plots\n",
    "    plt.plot(\n",
    "        batch_sizes[: len(postgres_times)],\n",
    "        postgres_times,\n",
    "        marker=\"o\",\n",
    "        label=\"PostgreSQL\",\n",
    "        color=\"blue\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "    plt.plot(\n",
    "        batch_sizes[: len(mongo_times)],\n",
    "        mongo_times,\n",
    "        marker=\"s\",\n",
    "        label=\"MongoDB\",\n",
    "        color=\"green\",\n",
    "        linewidth=2\n",
    "    )\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.title(\"Database Insertion Time Comparison\", fontsize=14)\n",
    "    plt.xlabel(\"Batch size\", fontsize=12)\n",
    "    plt.ylabel(\"Time (seconds)\", fontsize=12)\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "    plt.legend(fontsize=10)\n",
    "\n",
    "    # Use log scale for x-axis\n",
    "    plt.xscale(\"log\", base=2)\n",
    "\n",
    "    # Define custom tick positions\n",
    "    custom_ticks = batch_sizes\n",
    "    plt.xticks(custom_ticks, custom_ticks, rotation=45, ha=\"right\")\n",
    "\n",
    "    # Add value annotations\n",
    "    for i, (size, time) in enumerate(\n",
    "        zip(batch_sizes[: len(postgers_times)], postgers_times)\n",
    "    ):\n",
    "        plt.annotate(\n",
    "            f\"{time:.1f}s\",\n",
    "            (size, time),\n",
    "            textcoords=\"offset points\",\n",
    "            xytext=(0, 10),\n",
    "            ha=\"center\",\n",
    "            fontsize=8,\n",
    "        )\n",
    "\n",
    "    for i, (size, time) in enumerate(zip(batch_sizes[: len(mongo_times)], mongo_times)):\n",
    "        plt.annotate(\n",
    "            f\"{time:.1f}s\",\n",
    "            (size, time),\n",
    "            textcoords=\"offset points\",\n",
    "            xytext=(0, -15),\n",
    "            ha=\"center\",\n",
    "            fontsize=8,\n",
    "        )\n",
    "\n",
    "    # Add throughput information in a text box\n",
    "    postgres_throughput = [\n",
    "        metrics[\"rows_per_second\"] for metrics in postgres_metrics.values()\n",
    "    ]\n",
    "    mongo_throughput = [\n",
    "        metrics[\"rows_per_second\"] for metrics in mongo_metrics.values()\n",
    "    ]\n",
    "\n",
    "    text_info = (\n",
    "        f\"Max Throughput:\\n\"\n",
    "        f\"PostgreSQL: {max(postgres_throughput):.0f} rows/s\\n\"\n",
    "        f\"MongoDB: {max(mongo_throughput):.0f} rows/s\"\n",
    "    )\n",
    "\n",
    "    plt.text(\n",
    "        0.02,\n",
    "        0.98,\n",
    "        text_info,\n",
    "        transform=plt.gca().transAxes,\n",
    "        bbox=dict(facecolor=\"white\", alpha=0.8),\n",
    "        verticalalignment=\"top\",\n",
    "        fontsize=10,\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137ae99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_combined_insertion_metrics(postgres_metrics, mongo_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0eabe0",
   "metadata": {},
   "source": [
    "### 2. Semantic Search with PostgreSQL and PgVector Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f854721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import random\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from statistics import mean, stdev\n",
    "\n",
    "import psycopg\n",
    "from pgvector.psycopg import register_vector\n",
    "\n",
    "\n",
    "def performance_analysis_search_postgres(\n",
    "        connection,\n",
    "        search_fn,\n",
    "        result_key,\n",
    "        database_type=\"PostgreSQL\",\n",
    "        num_queries=100,\n",
    "        concurrent_queries=[1, 10, 50, 100],\n",
    "):\n",
    "    \"\"\"\n",
    "    Performance Analysis PostgreSQL search performance (vector or hybrid) with concurrent queries.\n",
    "\n",
    "    Args:\n",
    "        connection: Database connection.\n",
    "        search_fn: The search function to analyse.\n",
    "        result_key: Key for storing results ('vector' or 'hybrid').\n",
    "        database_type: Type of database (default 'PostgreSQL').\n",
    "        num_queries: Number of performance analysis iterations.\n",
    "        concurrent_queries: List of concurrency levels to test. \n",
    "    \"\"\"\n",
    "    query_templates = [\n",
    "        \"When was YouTube officially launched, and by whom?\",\n",
    "        \"What is YouTube's slogan introduced after Google's acquisition?\",\n",
    "        \"How many hours of videos are collectively watched on YouTube daily?\",\n",
    "        \"Which was the first video uploaded to YouTube, and when was it uploaded?\",\n",
    "        \"What was the acquisition cost of YouTube by Google, and when was the deal finalized?\",\n",
    "        \"What was the first YouTube video to reach one million views, and when did it happen?\",\n",
    "        \"What are the three separate branches of the United States government?\",\n",
    "        \"Which country has the highest documented incarceration rate and prison population?\",\n",
    "        \"How many executions have occurred in the United States since 1977, and which countries have more?\",\n",
    "        \"What percentage of the global military spending did the United States account for in 2019?\",\n",
    "        \"How is the U.S. president elected?\",\n",
    "        \"What cooling system innovation was included in the proposed venues for the World Cup in Qatar?\",\n",
    "        \"What lawsuit was filed against Google in June 2020, and what was it about?\",\n",
    "        \"How much was Google fined by CNIL in January 2022, and for what reason?\",\n",
    "        \"When did YouTube join the NSA's PRISM program, according to reports?\",\n",
    "    ]\n",
    "\n",
    "    # Initialize results structure if not already present.\n",
    "    if database_type not in performance_guidance_results:\n",
    "        performance_guidance_results[database_type] = {}\n",
    "    performance_guidance_results[database_type][result_key] = {}\n",
    "    performance_guidance_results[database_type][result_key][\"specific\"] = {}\n",
    "\n",
    "    def execute_single_query():\n",
    "        \"\"\"Execute a single query and measure its latency.\"\"\"\n",
    "        query = random.choice(query_templates)\n",
    "        start_time = time.time()\n",
    "        # Call the passed-in search function\n",
    "        search_fn(query, connection, top_n=5)\n",
    "        end_time = time.time()\n",
    "        return end_time - start_time\n",
    "    \n",
    "    # Run the performance analysis for each concurrency level.\n",
    "    for number_of_queries in concurrent_queries:\n",
    "        latencies = []\n",
    "        for _ in range(num_queries):\n",
    "            with ThreadPoolExecutor(max_workers=number_of_queries) as executor:\n",
    "                # Submit queries concurrently and collect latemcies.\n",
    "                futures = [\n",
    "                    executor.submit(execute_single_query)\n",
    "                    for _ in range(number_of_queries)\n",
    "                ]\n",
    "                batch_latencies = [\n",
    "                    future.result()\n",
    "                    for future in concurrent.futures.as_completed(futures)\n",
    "                ]\n",
    "                latencies.extend(batch_latencies)\n",
    "\n",
    "        avg_latency = mean(latencies)\n",
    "        throughput = 1 / avg_latency # Base queries per second per query.\n",
    "        p95_latency = sorted(latencies)[int(len(latencies) * 0.95)]\n",
    "        std_dev_latency = stdev(latencies)\n",
    "\n",
    "        performance_guidance_results[database_type][result_key][\"specific\"][\n",
    "            number_of_queries\n",
    "        ] = {\n",
    "            \"avg_latency\": avg_latency,\n",
    "            \"throughput\": throughput\n",
    "            * number_of_queries, # Scale throughput by concurrency.\n",
    "            \"p95_latency\": p95_latency,\n",
    "            \"std_dev\": std_dev_latency,\n",
    "        }\n",
    "\n",
    "    return performance_guidance_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54ec4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONCURRENT_QUERIES = [1, 2, 4, 5, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2490bc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the performance Analysis\n",
    "try:\n",
    "    conn = psycopg.connect(\n",
    "        f\"dbname={neon_db_database} user={neon_db_user} password={neon_db_password} host={neon_db_host}\"\n",
    "    )\n",
    "    register_vector(conn)\n",
    "\n",
    "    print(\"Running performance analysis for vector search...\")\n",
    "    # Run the performance analysis for vector and hybrid search\n",
    "    performance_analysis_search_postgres(\n",
    "        conn,\n",
    "        vector_search_with_postgres,\n",
    "        \"vector\",\n",
    "        database_type=\"PostgreSQL\",\n",
    "        num_queries=TOTAL_QUERIES,\n",
    "        concurrent_queries=CONCURRENT_QUERIES,\n",
    "    )\n",
    "\n",
    "    print(\"Running performance analysis for hybrid...\")\n",
    "    performance_analysis_search_postgres(\n",
    "        conn,\n",
    "        hybrid_search_with_postgres,\n",
    "        \"hybrid\",\n",
    "        database_type=\"PostgreSQL\",\n",
    "        num_queries=TOTAL_QUERIES,\n",
    "        concurrent_queries=CONCURRENT_QUERIES,\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Performance Analysis failed: \", e)\n",
    "finally:\n",
    "    conn.close()\n",
    "    print(\"\\nConnection closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8e84c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(performance_guidance_results[\"PostgreSQL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad99aba",
   "metadata": {},
   "source": [
    "#### 2.2. MongoDB Semantic Search Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b951c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_analysis_search_mongo(\n",
    "        collection,\n",
    "        search_fn,\n",
    "        result_key,\n",
    "        database_type=\"MongoDB\",\n",
    "        num_queries=100,\n",
    "        concurrent_queries=[1, 10, 50, 100],\n",
    "):\n",
    "    \"\"\"\n",
    "    MongoDB search performance (vector or hybrid) with true concurrency.\n",
    "\n",
    "    Args:\n",
    "        collection: MongoDB collection object.\n",
    "        search_fn: The search function to analyse. It should accept a query string.\n",
    "                    For example:\n",
    "                    - For vector search: lambda q: vector_search_with_mongodb(q, collection, top_n=5)\n",
    "                    - For hybrid search: lambda q: hybrid_search_with_mongodb(collection, q)\n",
    "        result_key: Key to store results under (e.g., \"vector\" or \"hybrid\").\n",
    "        database_type: Type of database (default: \"MongoDB\"). \n",
    "        num_queries: Number of performance analysis iterations for statistical significance.\n",
    "        concurrent_queries: Different concurrency levels to test.\n",
    "\n",
    "    Returns:\n",
    "        The updated performance analysis results for the specified database type.\n",
    "    \"\"\"\n",
    "    query_templates = [\n",
    "        \"When was YouTube officially launched, and by whom?\",\n",
    "        \"What is YouTube's slogan introduced after Google's acquisition?\",\n",
    "        \"How many hours of videos are collectively watched on YouTube daily?\",\n",
    "        \"Which was the first video uploaded to YouTube, and when was it uploaded?\",\n",
    "        \"What was the acquisition cost of YouTube by Google, and when was the deal finalized?\",\n",
    "        \"What was the first YouTube video to reach one million views, and when did it happen?\",\n",
    "        \"What are the three separate branches of the United States government?\",\n",
    "        \"Which country has the highest documented incarceration rate and prison population?\",\n",
    "        \"How many executions have occurred in the United States since 1977, and which countries have more?\",\n",
    "        \"What percentage of the global military spending did the United States account for in 2019?\",\n",
    "        \"How is the U.S. president elected?\",\n",
    "        \"What cooling system innovation was included in the proposed venues for the World Cup in Qatar?\",\n",
    "        \"What lawsuit was filed against Google in June 2020, and what was it about?\",\n",
    "        \"How much was Google fined by CNIL in January 2022, and for what reason?\",\n",
    "        \"When did YouTube join the NSA's PRISM program, according to reports?\",\n",
    "    ]\n",
    "\n",
    "    if database_type not in performance_guidance_results:\n",
    "        performance_guidance_results[database_type] = {}\n",
    "    performance_guidance_results[database_type][result_key] = {}\n",
    "    performance_guidance_results[database_type][result_key][\"specific\"] = {}\n",
    "\n",
    "    def execute_single_query():\n",
    "        \"\"\"Execute a single query using the provided search function and measure its latency.\"\"\"\n",
    "        query = random.choice(query_templates)\n",
    "        start_time = time.time()\n",
    "        # The search_fn is expected to handle the query and perform the search\n",
    "        result = search_fn(query, collection)\n",
    "        end_time = time.time()\n",
    "        return end_time - start_time\n",
    "    \n",
    "    # Loop over each concurrency level and accumulate latencies.\n",
    "    for number_of_queries in concurrent_queries:\n",
    "        latencies = []\n",
    "        for _ in range(num_queries):\n",
    "            with ThreadPoolExecutor(max_workers=number_of_queries) as executor:\n",
    "                # Submit multiple queries concurrently.\n",
    "                futures = [\n",
    "                    executor.submit(execute_single_query)\n",
    "                    for _ in range(number_of_queries)\n",
    "                ]\n",
    "                # Collect latencies as queries complete.\n",
    "                batch_latencies = [\n",
    "                    future.result()\n",
    "                    for future in concurrent.futures.as_completed(futures)\n",
    "                ]\n",
    "                latencies.extend(batch_latencies)\n",
    "\n",
    "        # Calculate performance analysis metrics.\n",
    "        avg_latency = mean(latencies)\n",
    "        throughput = 1 / avg_latency # Base queries per second per query.\n",
    "        p95_latency = sorted(latencies)[int(len(latencies) * 0.95)]\n",
    "        std_dev_latency = stdev(latencies)\n",
    "\n",
    "        performance_guidance_results[database_type][result_key][\"specific\"][number_of_queries] = {\n",
    "            \"avg_latency\": avg_latency,\n",
    "            \"throughtput\": throughput\n",
    "            * number_of_queries,   # Scale throughput by concurrency.\n",
    "            \"p95_latency\": p95_latency,\n",
    "            \"std_dev\": std_dev_latency, \n",
    "        }\n",
    "    return performance_guidance_results[database_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e639605b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the performance analysis for MongoDB\n",
    "try:\n",
    "    mongo_client = get_mongo_client(MONGO_URI)\n",
    "    db = mongo_client[DB_NAME]\n",
    "    collection = db[COLLECTION_NAME]\n",
    "\n",
    "    print(\"Running MongoDB performance analysis for vector search...\")\n",
    "    performance_analysis_search_mongo(\n",
    "        collection,\n",
    "        search_fn=vector_search_with_mongodb,\n",
    "        result_key=\"vector\",\n",
    "        num_queries=TOTAL_QUERIES,\n",
    "        concurrent_queries=CONCURRENT_QUERIES,\n",
    "    )\n",
    "\n",
    "    print(\"Running MongoDB performance analysis for hybrid search...\")\n",
    "    performance_analysis_search_mongo(\n",
    "        collection,\n",
    "        search_fn=hybrid_search_with_mongodb,\n",
    "        result_key=\"hybrid\",\n",
    "        num_queries=TOTAL_QUERIES,\n",
    "        concurrent_queries=CONCURRENT_QUERIES,\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"MongoDB performance analysis failed:\", e)\n",
    "finally:\n",
    "    mongo_client.close()\n",
    "    print(\"\\nMongoDB connection closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1b8cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(performance_guidance_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4e7560",
   "metadata": {},
   "source": [
    "#### 2.3 Visualize Vector Search Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4f4e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_chart_performance_analysis_comparison(\n",
    "        performance_guidance_results,\n",
    "        metric=\"avg_latency\",\n",
    "        metric_label=\"Average Latency (ms)\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates bar charts to compare performance analysis results for each metric across databases,\n",
    "    for each search type (\"vector\" and \"hybrid\") using the 'specific' results. \n",
    "    \"\"\"\n",
    "    search_types = [\"vector\", \"hybrid\"]\n",
    "    batch_sizes = sorted(\n",
    "        list(\n",
    "            next(iter(performance_guidance_results.values()))[\"vector\"][\n",
    "                \"specific\"\n",
    "            ].keys()\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Loop over each search type\n",
    "    for search_type in search_types:\n",
    "        # Build data for this search type\n",
    "        data = []\n",
    "        for batch_size in batch_sizes:\n",
    "            row = {\"Batch Size\": batch_size}\n",
    "            for db_type in performance_guidance_results:\n",
    "                try:\n",
    "                    value = (\n",
    "                        performance_guidance_results[db_type]\n",
    "                        .get(search_type, {})\n",
    "                        .get(\"specific\", {})\n",
    "                        .get(batch_size, {})\n",
    "                        .get(metric, 0)\n",
    "                    )\n",
    "                    # If metric is a latency, convert to ms\n",
    "                    if value is not None and metric in [\"avg_latency\", \"p95_latency\"]:\n",
    "                        value *= 1000 # Convert seconds to ms\n",
    "                    row[db_type] = value if value is not None else 0\n",
    "                except Exception:\n",
    "                    row[db_type] = 0\n",
    "            data.append(row)\n",
    "\n",
    "        # Prepare labels and values from the data\n",
    "        labels = [str(row[\"Batch Size\"]) for row in data]\n",
    "        postgres_values = [row.get(\"PostgreSQL\", 0) for row in data]\n",
    "        mongodb_values = [row.get(\"MongoDB\", 0) for row in data]\n",
    "\n",
    "        # Create the bar chart for this search type\n",
    "        fig, ax = plt.subplots(figsize=(15, 6))\n",
    "        width = 0.35\n",
    "        x = np.arange(len(labels))\n",
    "\n",
    "        postgres_bars = ax.bar(\n",
    "            x - width / 2,\n",
    "            postgres_values,\n",
    "            width,\n",
    "            label=\"PostgreSQL\",\n",
    "            color=\"lightblue\",\n",
    "            edgecolor=\"blue\",\n",
    "        )\n",
    "        mongodb_bars = ax.bar(\n",
    "            x + width / 2,\n",
    "            mongodb_values,\n",
    "            width,\n",
    "            label=\"MongoDB\",\n",
    "            color=\"lightgreen\",\n",
    "            edgecolor=\"green\",\n",
    "        )\n",
    "\n",
    "        ax.grid(True, linestyle=\"--\", alpha=0.7, axis=\"y\")\n",
    "        ax.set_title(\n",
    "            f\"{metric_label} Comparison for {search_type.capitalize()} Search\", pad=20\n",
    "        )\n",
    "        ax.set_xlabel(\"Concurrent Queries\", labelpad=10)\n",
    "        ax.set_ylabel(metric_label, labelpad=10)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "        ax.legend(forntsize=10)\n",
    "\n",
    "        def autolabel(rects):\n",
    "            for rect in rects:\n",
    "                height = rect.get_height()\n",
    "                if height > 0:\n",
    "                    ax.annotate(\n",
    "                        f\"{height:.2f}\",\n",
    "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                        xytext=(0, 3),\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha=\"center\",\n",
    "                        va=\"bottom\",\n",
    "                        rotation=90,\n",
    "                        fontsize=8,\n",
    "                    )\n",
    "\n",
    "        autolabel(postgres_bars)\n",
    "        autolabel(mongodb_bars)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f25054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate bar charts for each metric\n",
    "bar_chart_performance_analysis_comparison(\n",
    "    performance_guidance_results,\n",
    "    metric=\"avg_latency\",\n",
    "    metric_label=\"Average Latency (ms)\",\n",
    ")\n",
    "bar_chart_performance_analysis_comparison(\n",
    "    performance_guidance_results,\n",
    "    metric=\"throughput\",\n",
    "    metric_label=\"Throughput (queries/sec)\",\n",
    ")\n",
    "bar_chart_performance_analysis_comparison(\n",
    "    performance_guidance_results,\n",
    "    metric=\"p95_latency\",\n",
    "    metric_label=\"P95 Latency (ms)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9298e80f",
   "metadata": {},
   "source": [
    "## Part 5: Extra Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cef1fce",
   "metadata": {},
   "source": [
    "5.1 PostgreSQL JSONB vs. MongoDB BSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe4a880",
   "metadata": {},
   "source": [
    "5.2 Limitations of pgvector for Handling Large-Dimensional Embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
